{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3b9ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cali-recod/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "from transformers import Sam3Processor, Sam3Model\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6415892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "forged_dir = \"train_images/forged\"\n",
    "authentic_dir = \"train_images/authentic\"\n",
    "mask_dir = \"train_masks\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "training_size = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbdbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization functions\n",
    "def display_image_with_mask(\n",
    "    image: Image.Image,\n",
    "    mask: np.ndarray,\n",
    "    opacity: float = 0.5,\n",
    "    color: str = \"red\",\n",
    "    only_outline: bool = True,\n",
    "    separate_images: bool = False,\n",
    "    show_labels: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Display an image with a mask overlay.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): The image to display.\n",
    "        mask (np.ndarray): Binary mask(s) to overlay on the image.\n",
    "        opacity (float): The opacity of the mask overlay.\n",
    "        color (str): The color of the mask overlay.\n",
    "        only_outline (bool): Whether to only display the outline of the mask.\n",
    "        separate_images (bool): If True, display each mask on a separate image.\n",
    "                                If False, display all masks on one image.\n",
    "        show_labels (bool): If True, display object index numbers at centroids.\n",
    "    \"\"\"\n",
    "    img_array = np.array(image)\n",
    "    color_rgb = matplotlib.colors.to_rgb(color)\n",
    "\n",
    "    if not separate_images:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.imshow(img_array)\n",
    "\n",
    "    for i in range(mask.shape[0]):\n",
    "        if separate_images:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "            ax.imshow(img_array)\n",
    "\n",
    "        mask_i = mask[i].astype(np.uint8)\n",
    "\n",
    "        if only_outline:\n",
    "            contours, _ = cv2.findContours(mask_i, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            for contour in contours:\n",
    "                contour = contour.squeeze()\n",
    "                if len(contour.shape) == 2:\n",
    "                    ax.plot(contour[:, 0], contour[:, 1], color=color, linewidth=2)\n",
    "        else:\n",
    "            overlay = np.zeros((*mask_i.shape, 4))\n",
    "            overlay[mask_i == 1] = [*color_rgb, opacity]\n",
    "            ax.imshow(overlay)\n",
    "\n",
    "        if show_labels:\n",
    "            ys, xs = np.where(mask_i)\n",
    "            if len(xs) > 0:\n",
    "                cx, cy = xs.mean(), ys.mean()\n",
    "                ax.text(cx, cy, str(i), fontsize=10, color='white', fontweight='bold',\n",
    "                        ha='center', va='center', bbox=dict(boxstyle='round,pad=0.2', \n",
    "                        facecolor='black', alpha=0.7))\n",
    "\n",
    "        if separate_images:\n",
    "            ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    if not separate_images:\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self,\n",
    "                 image: Image.Image,\n",
    "                 mask: np.ndarray,\n",
    "                 sam3_results: dict = None,\n",
    "                 hu_moments: np.ndarray = None):\n",
    "        self.image = image\n",
    "        self.ground_truth_mask = mask\n",
    "        self.sam3_results = sam3_results\n",
    "        self.hu_moments = hu_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "forged_image_paths = sorted([f for f in os.listdir(forged_dir)])\n",
    "forged_mask_paths = sorted([f for f in os.listdir(mask_dir)])\n",
    "authentic_image_paths = sorted([f for f in os.listdir(authentic_dir)])\n",
    "\n",
    "forged_images = [Image.open(os.path.join(forged_dir, f)) for f in forged_image_paths]\n",
    "forged_masks = [np.load(os.path.join(mask_dir, f)) for f in forged_mask_paths]\n",
    "authentic_images = [Image.open(os.path.join(authentic_dir, f)) for f in authentic_image_paths]\n",
    "\n",
    "forged_samples = list(zip(forged_images, forged_masks))\n",
    "authentic_samples = list(zip(authentic_images, [None for _ in authentic_images]))\n",
    "all_samples = forged_samples + authentic_samples\n",
    "all_samples = [Sample(image, mask) for image, mask in all_samples]\n",
    "\n",
    "# shuffle the samples\n",
    "random.Random(42).shuffle(all_samples)\n",
    "\n",
    "print(\"forged set: \", len(forged_samples))\n",
    "print(\"authentic set: \", len(authentic_samples))\n",
    "\n",
    "train_samples, test_samples = train_test_split(all_samples, test_size=1-training_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for embedding models (loaded on-demand to save GPU memory)\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoImageProcessor, AutoModel, AutoProcessor\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ed76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n",
    "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sam3_results(\n",
    "    model: Sam3Model,\n",
    "    processor: Sam3Processor,\n",
    "    samples: list[Sample],\n",
    "    batch_size: int = 1\n",
    ") -> list[Sample]:\n",
    "\n",
    "    new_samples = []\n",
    "\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        batch = samples[i:i+batch_size]\n",
    "        images = [sample.image for sample in batch]\n",
    "        masks = [sample.ground_truth_mask for sample in batch]\n",
    "\n",
    "        inputs = processor(images=images, text=[\"distinct object\"] * len(images), return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Use library's post-processor\n",
    "            results_batch = processor.post_process_instance_segmentation(\n",
    "                outputs,\n",
    "                threshold=0.20,\n",
    "                mask_threshold=0.5,\n",
    "                target_sizes=inputs.get(\"original_sizes\").tolist()\n",
    "            )\n",
    "            \n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        for img, mask, results in zip(images, masks, results_batch):\n",
    "            new_samples.append(Sample(\n",
    "                image=img,\n",
    "                mask=mask,\n",
    "                sam3_results=results\n",
    "            ))\n",
    "        \n",
    "        del inputs, results_batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return new_samples\n",
    "\n",
    "def get_masked_crop(image, mask):\n",
    "    \"\"\"Extract bounding box crop of masked region, filling outside with inverse of average color\"\"\"\n",
    "    mask_np = mask.cpu().numpy()\n",
    "    ys, xs = np.where(mask_np)\n",
    "    if len(xs) == 0:\n",
    "        return None\n",
    "    \n",
    "    img_array = np.array(image.convert(\"RGB\"))\n",
    "    \n",
    "    # Calculate average color of masked region, then invert it\n",
    "    avg_color = img_array[mask_np == 1].mean(axis=0)\n",
    "    inv_color = 255 - avg_color\n",
    "    \n",
    "    # Fill outside mask with inverse color\n",
    "    masked_img = img_array.copy()\n",
    "    masked_img[mask_np == 0] = inv_color\n",
    "    \n",
    "    # Crop to bounding box\n",
    "    crop = Image.fromarray(masked_img[ys.min():ys.max()+1, xs.min():xs.max()+1].astype(np.uint8))\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_with_sam3 = get_sam3_results(model, processor, train_samples[:100])\n",
    "\n",
    "# Free SAM3 from GPU\n",
    "del model\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "sample_x = train_samples_with_sam3[num]\n",
    "\n",
    "print(f\"Found {len(sample_x.sam3_results['masks'])} objects\")\n",
    "\n",
    "display_image_with_mask(\n",
    "    sample_x.image,\n",
    "    sample_x.ground_truth_mask,\n",
    "    # opacity=0.0,\n",
    "    # only_outline=False\n",
    ")\n",
    "display_image_with_mask(\n",
    "    sample_x.image,\n",
    "    sample_x.sam3_results[\"masks\"].cpu().numpy(),\n",
    "    show_labels=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hu_moments(samples: list[Sample]):\n",
    "    for sample in samples:\n",
    "        masks = sample.sam3_results[\"masks\"].cpu().numpy().astype(np.uint8)\n",
    "        hu_moments_list = []\n",
    "        for i in range(masks.shape[0]):\n",
    "            mask_np = masks[i]  # Get single 2D mask\n",
    "            moments = cv2.moments(mask_np)\n",
    "            hu = cv2.HuMoments(moments).flatten()\n",
    "\n",
    "            # Sign-preserving log transformation\n",
    "            hu_log = np.sign(hu) * np.log10(np.abs(hu) + 1e-10)\n",
    "            hu_moments_list.append(np.round(hu_log, 4))\n",
    "        sample.hu_moments = np.array(hu_moments_list)\n",
    "    return samples\n",
    "\n",
    "train_samples_with_hu_moments = add_hu_moments(train_samples_with_sam3)\n",
    "\n",
    "def find_closest_by_hu_moments(hu_moments: np.ndarray, obj_idx: int, k: int = 10) -> list[tuple[int, float]]:\n",
    "    \"\"\"Find k closest objects to obj_idx by Hu moment distance. Returns [(idx, distance), ...]\"\"\"\n",
    "    distances = [(i, np.linalg.norm(hu_moments[obj_idx] - hu_moments[i])) \n",
    "                 for i in range(len(hu_moments)) if i != obj_idx]\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances[:k]\n",
    "\n",
    "def find_closest_by_embeddings(embeddings: list, obj_idx: int, k: int = 10) -> list[tuple[int, float]]:\n",
    "    \"\"\"Find k closest objects to obj_idx by cosine distance. Returns [(idx, distance), ...]\"\"\"\n",
    "    if embeddings[obj_idx] is None:\n",
    "        return []\n",
    "    distances = []\n",
    "    for i in range(len(embeddings)):\n",
    "        if i == obj_idx or embeddings[i] is None:\n",
    "            continue\n",
    "        sim = np.dot(embeddings[obj_idx], embeddings[i]) / (np.linalg.norm(embeddings[obj_idx]) * np.linalg.norm(embeddings[i]))\n",
    "        distances.append((i, 1 - sim))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances[:k]\n",
    "\n",
    "sample_x = train_samples_with_hu_moments[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_obj = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 10 closest objects to object 0 by Hu moments\n",
    "closest = find_closest_by_hu_moments(sample_x.hu_moments, query_obj)\n",
    "print(f\"Hu moments - 10 closest to object {query_obj}:\")\n",
    "for idx, dist in closest:\n",
    "    print(f\"  {idx}: {dist:.4f}\")\n",
    "\n",
    "# Display query object and its closest matches\n",
    "masks = sample_x.sam3_results[\"masks\"].cpu().numpy()\n",
    "display_image_with_mask(sample_x.image, np.array([masks[query_obj]] + [masks[idx] for idx, _ in closest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfe391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP Base\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Get CLIP embeddings for each object\n",
    "clip_embeddings = []\n",
    "for mask in sample_x.sam3_results[\"masks\"]:\n",
    "    crop = get_masked_crop(sample_x.image, mask)\n",
    "    if crop is None:\n",
    "        clip_embeddings.append(None)\n",
    "        continue\n",
    "    inputs = clip_processor(images=crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.get_image_features(**inputs)\n",
    "    clip_embeddings.append(emb[0].cpu().numpy())\n",
    "\n",
    "# Free CLIP Base\n",
    "del clip_model, clip_processor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 10 closest objects by CLIP\n",
    "closest = find_closest_by_embeddings(clip_embeddings, query_obj)\n",
    "print(f\"CLIP - 10 closest to object {query_obj}:\")\n",
    "for idx, dist in closest:\n",
    "    print(f\"  {idx}: {dist:.4f}\")\n",
    "\n",
    "masks = sample_x.sam3_results[\"masks\"].cpu().numpy()\n",
    "display_image_with_mask(sample_x.image, np.array([masks[query_obj]] + [masks[idx] for idx, _ in closest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP Large\n",
    "clip_large_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_large_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Get CLIP Large embeddings for each object\n",
    "clip_large_embeddings = []\n",
    "for mask in sample_x.sam3_results[\"masks\"]:\n",
    "    crop = get_masked_crop(sample_x.image, mask)\n",
    "    if crop is None:\n",
    "        clip_large_embeddings.append(None)\n",
    "        continue\n",
    "    inputs = clip_large_processor(images=crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_large_model.get_image_features(**inputs)\n",
    "    clip_large_embeddings.append(emb[0].cpu().numpy())\n",
    "\n",
    "# Free CLIP Large\n",
    "del clip_large_model, clip_large_processor\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Find 10 closest objects by CLIP Large\n",
    "closest = find_closest_by_embeddings(clip_large_embeddings, query_obj)\n",
    "print(f\"CLIP Large - 10 closest to object {query_obj}:\")\n",
    "for idx, dist in closest:\n",
    "    print(f\"  {idx}: {dist:.4f}\")\n",
    "\n",
    "masks = sample_x.sam3_results[\"masks\"].cpu().numpy()\n",
    "display_image_with_mask(sample_x.image, np.array([masks[query_obj]] + [masks[idx] for idx, _ in closest]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcbb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db284904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv2\n",
    "dino_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "dino_model = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
    "\n",
    "# Get DINOv2 embeddings for each object\n",
    "dino_embeddings = []\n",
    "for mask in sample_x.sam3_results[\"masks\"]:\n",
    "    crop = get_masked_crop(sample_x.image, mask)\n",
    "    if crop is None:\n",
    "        dino_embeddings.append(None)\n",
    "        continue\n",
    "    inputs = dino_processor(images=crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = dino_model(**inputs)\n",
    "        emb = outputs.last_hidden_state.mean(dim=1)[0]  # Pool over patches\n",
    "    dino_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "# Free DINOv2\n",
    "del dino_model, dino_processor\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Find 10 closest objects by DINOv2\n",
    "closest = find_closest_by_embeddings(dino_embeddings, query_obj)\n",
    "print(f\"DINOv2 - 10 closest to object {query_obj}:\")\n",
    "for idx, dist in closest:\n",
    "    print(f\"  {idx}: {dist:.4f}\")\n",
    "\n",
    "masks = sample_x.sam3_results[\"masks\"].cpu().numpy()\n",
    "display_image_with_mask(sample_x.image, np.array([masks[query_obj]] + [masks[idx] for idx, _ in closest]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SigLIP\n",
    "siglip_processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "siglip_model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").to(device)\n",
    "\n",
    "# Get SigLIP embeddings for each object\n",
    "siglip_embeddings = []\n",
    "for mask in sample_x.sam3_results[\"masks\"]:\n",
    "    crop = get_masked_crop(sample_x.image, mask)\n",
    "    if crop is None:\n",
    "        siglip_embeddings.append(None)\n",
    "        continue\n",
    "    inputs = siglip_processor(images=crop, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = siglip_model.get_image_features(**inputs)\n",
    "    siglip_embeddings.append(outputs[0].cpu().numpy())\n",
    "\n",
    "# Free SigLIP\n",
    "del siglip_model, siglip_processor\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Find 10 closest objects by SigLIP\n",
    "closest = find_closest_by_embeddings(siglip_embeddings, query_obj)\n",
    "print(f\"SigLIP - 10 closest to object {query_obj}:\")\n",
    "for idx, dist in closest:\n",
    "    print(f\"  {idx}: {dist:.4f}\")\n",
    "\n",
    "masks = sample_x.sam3_results[\"masks\"].cpu().numpy()\n",
    "display_image_with_mask(sample_x.image, np.array([masks[query_obj]] + [masks[idx] for idx, _ in closest]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EVA-CLIP\n",
    "eva_model, _, eva_preprocess = open_clip.create_model_and_transforms('EVA02-B-16', pretrained='merged2b_s8b_b131k')\n",
    "eva_model = eva_model.to(device)\n",
    "\n",
    "# Get EVA-CLIP embeddings for each object\n",
    "eva_embeddings = []\n",
    "for mask in sample_x.sam3_results[\"masks\"]:\n",
    "    crop = get_masked_crop(sample_x.image, mask)\n",
    "    if crop is None:\n",
    "        eva_embeddings.append(None)\n",
    "        continue\n",
    "    img_tensor = eva_preprocess(crop).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = eva_model.encode_image(img_tensor)\n",
    "    eva_embeddings.append(emb[0].cpu().numpy())\n",
    "\n",
    "# Free EVA-CLIP\n",
    "del eva_model, eva_preprocess\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Find 10 closest objects by EVA-CLIP\n",
    "closest = find_closest_by_embeddings(eva_embeddings, query_obj)\n",
    "print(f\"EVA-CLIP - 10 closest to object {query_obj}:\")\n",
    "for idx, dist in closest:\n",
    "    print(f\"  {idx}: {dist:.4f}\")\n",
    "\n",
    "masks = sample_x.sam3_results[\"masks\"].cpu().numpy()\n",
    "display_image_with_mask(sample_x.image, np.array([masks[query_obj]] + [masks[idx] for idx, _ in closest]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
