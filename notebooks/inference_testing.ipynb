{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ðŸ”§ CONFIGURATION - Edit these variables to test different settings\n",
        "# ============================================================================\n",
        "\n",
        "# --- Model & Weights ---\n",
        "USE_KAGGLE_WEIGHTS = False\n",
        "KAGGLE_WEIGHTS_PATH = \"/kaggle/input/recod-1219/best_model.pt\"\n",
        "LOCAL_WEIGHTS_PATH = \"../checkpoints/best_model.pt\"\n",
        "DINO_PATH = \"facebook/dinov2-base\"  # or \"/kaggle/input/dinov2/pytorch/base/1\" on Kaggle\n",
        "IMG_SIZE = 512\n",
        "CHANNELS = 4\n",
        "UNFREEZE_BLOCKS = 3\n",
        "DECODER_DROPOUT = 0.05\n",
        "\n",
        "# --- Data ---\n",
        "DATASET_ID = \"eliplutchok/recod-finetune\"\n",
        "SAMPLE_SIZE = 100  # None = all\n",
        "\n",
        "# --- Post-Processing (re-run evaluation cell after changing these) ---\n",
        "USE_ENHANCED_ADAPTIVE = True  # False = simple threshold\n",
        "ALPHA_GRAD = 0.35\n",
        "GAUSSIAN_BLUR_SIZE = 3\n",
        "THRESHOLD_STD_MULT = 0.3\n",
        "SIMPLE_THRESHOLD = 0.5\n",
        "USE_MORPHOLOGY = True\n",
        "MORPH_CLOSE_KERNEL = 5\n",
        "MORPH_OPEN_KERNEL = 3\n",
        "MIN_AREA = 400\n",
        "MIN_MEAN_PROB = 0.35\n",
        "\n",
        "WEIGHTS_PATH = KAGGLE_WEIGHTS_PATH if USE_KAGGLE_WEIGHTS else LOCAL_WEIGHTS_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup, Model Definition & Loading\n",
        "import os, cv2, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class DinoDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=768, out_channels=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.up1 = self._block(in_channels, 384, dropout)\n",
        "        self.up2 = self._block(384, 192, dropout)\n",
        "        self.up3 = self._block(192, 96, dropout)\n",
        "        self.up4 = self._block(96, 48, dropout)\n",
        "        self.final = nn.Conv2d(48, out_channels, kernel_size=1)\n",
        "    \n",
        "    def _block(self, in_ch, out_ch, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(dropout), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n",
        "    \n",
        "    def forward(self, features, target_size):\n",
        "        x = F.interpolate(features, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up1(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up2(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up3(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "        x = self.up4(x)\n",
        "        return self.final(x)\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, backbone, out_channels=4, unfreeze_blocks=3, decoder_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(backbone)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "        for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        for i in range(len(self.encoder.encoder.layer) - unfreeze_blocks, len(self.encoder.encoder.layer)):\n",
        "            for p in self.encoder.encoder.layer[i].parameters(): p.requires_grad = True\n",
        "        for p in self.encoder.layernorm.parameters(): p.requires_grad = True\n",
        "        self.decoder = DinoDecoder(hidden_size, out_channels, decoder_dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x_norm = (x - self.mean) / self.std\n",
        "        feats = self.encoder(pixel_values=x_norm).last_hidden_state\n",
        "        B, N, C = feats.shape\n",
        "        fmap = feats[:, 1:, :].permute(0, 2, 1).reshape(B, C, int(math.sqrt(N-1)), int(math.sqrt(N-1)))\n",
        "        target_size = (x.shape[2], x.shape[3])\n",
        "        return self.decoder(fmap, target_size)\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Loading weights from: {WEIGHTS_PATH}\")\n",
        "model = DinoSegmenter(DINO_PATH, CHANNELS, UNFREEZE_BLOCKS, DECODER_DROPOUT).to(device)\n",
        "ckpt = torch.load(WEIGHTS_PATH, map_location=device, weights_only=False)\n",
        "model.load_state_dict(ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt)\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset and cache all model outputs (run once, then test different post-processing)\n",
        "print(f\"Loading dataset: {DATASET_ID}\")\n",
        "dataset = load_dataset(DATASET_ID, split=\"train\")\n",
        "if SAMPLE_SIZE and SAMPLE_SIZE < len(dataset):\n",
        "    random.seed(42)\n",
        "    dataset = dataset.select(random.sample(range(len(dataset)), SAMPLE_SIZE))\n",
        "print(f\"Testing on {len(dataset)} samples\")\n",
        "\n",
        "# Cache: stores (probs, original_size, gt_masks) for each sample\n",
        "cache = []\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_probs(pil_img):\n",
        "    img = pil_img.resize((IMG_SIZE, IMG_SIZE))\n",
        "    x = torch.from_numpy(np.array(img, np.float32) / 255.).permute(2, 0, 1)[None].to(device)\n",
        "    return torch.sigmoid(model(x))[0].cpu().numpy()\n",
        "\n",
        "print(\"Running model on all samples (this only needs to run once)...\")\n",
        "for idx in tqdm(range(len(dataset))):\n",
        "    example = dataset[idx]\n",
        "    img = example[\"image\"].convert(\"RGB\")\n",
        "    original_size = img.size\n",
        "    \n",
        "    gt_masks = example.get(\"mask\")\n",
        "    if gt_masks and isinstance(gt_masks, list):\n",
        "        gt_masks = [np.array(m).astype(np.uint8) for m in gt_masks]\n",
        "    else:\n",
        "        gt_masks = []\n",
        "    \n",
        "    probs = get_probs(img)\n",
        "    cache.append({\"probs\": probs, \"size\": original_size, \"gt\": gt_masks})\n",
        "\n",
        "print(f\"âœ… Cached {len(cache)} samples. Now you can re-run the evaluation cell with different settings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âš¡ EVALUATE - Re-run this cell after changing post-processing settings in cell 0\n",
        "def process_probs(probs, size):\n",
        "    \"\"\"Apply current post-processing settings to cached probs.\"\"\"\n",
        "    masks = []\n",
        "    for ch in range(probs.shape[0]):\n",
        "        p = probs[ch]\n",
        "        if USE_ENHANCED_ADAPTIVE:\n",
        "            gx, gy = cv2.Sobel(p, cv2.CV_32F, 1, 0, ksize=3), cv2.Sobel(p, cv2.CV_32F, 0, 1, ksize=3)\n",
        "            grad = np.sqrt(gx**2 + gy**2) / (np.sqrt(gx**2 + gy**2).max() + 1e-6)\n",
        "            enhanced = cv2.GaussianBlur((1 - ALPHA_GRAD) * p + ALPHA_GRAD * grad, (GAUSSIAN_BLUR_SIZE, GAUSSIAN_BLUR_SIZE), 0)\n",
        "            thr = np.mean(enhanced) + THRESHOLD_STD_MULT * np.std(enhanced)\n",
        "            mask = (enhanced > thr).astype(np.uint8)\n",
        "        else:\n",
        "            mask = (p > SIMPLE_THRESHOLD).astype(np.uint8)\n",
        "        \n",
        "        if USE_MORPHOLOGY:\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((MORPH_CLOSE_KERNEL, MORPH_CLOSE_KERNEL), np.uint8))\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((MORPH_OPEN_KERNEL, MORPH_OPEN_KERNEL), np.uint8))\n",
        "        \n",
        "        mask = cv2.resize(mask, size, interpolation=cv2.INTER_NEAREST)\n",
        "        area = mask.sum()\n",
        "        if area >= MIN_AREA:\n",
        "            prob_resized = cv2.resize(p, size, interpolation=cv2.INTER_LINEAR)\n",
        "            if area > 0 and prob_resized[mask == 1].mean() >= MIN_MEAN_PROB:\n",
        "                masks.append(mask)\n",
        "    return masks\n",
        "\n",
        "def compute_metrics(pred, gt):\n",
        "    intersection = np.logical_and(pred, gt).sum()\n",
        "    union = np.logical_or(pred, gt).sum()\n",
        "    iou = intersection / union if union > 0 else 1.0\n",
        "    dice = 2 * intersection / (pred.sum() + gt.sum()) if (pred.sum() + gt.sum()) > 0 else 1.0\n",
        "    tp, fp, fn = intersection, (pred & ~gt).sum(), (~pred & gt).sum()\n",
        "    prec, rec = tp / (tp + fp + 1e-8), tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
        "    return iou, dice, prec, rec, f1\n",
        "\n",
        "results = []\n",
        "for idx, item in enumerate(cache):\n",
        "    probs, size, gt_masks = item[\"probs\"], item[\"size\"], item[\"gt\"]\n",
        "    pred_masks = process_probs(probs, size)\n",
        "    \n",
        "    h, w = size[1], size[0]\n",
        "    pred = np.zeros((h, w), dtype=bool)\n",
        "    for m in pred_masks: pred |= m.astype(bool)\n",
        "    gt = np.zeros((h, w), dtype=bool)\n",
        "    for m in gt_masks:\n",
        "        if m.shape == (h, w): gt |= m.astype(bool)\n",
        "    \n",
        "    iou, dice, prec, rec, f1 = compute_metrics(pred, gt)\n",
        "    results.append({\"idx\": idx, \"iou\": iou, \"dice\": dice, \"precision\": prec, \"recall\": rec, \"f1\": f1, \n",
        "                    \"n_pred\": len(pred_masks), \"n_gt\": len(gt_masks)})\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Print results\n",
        "print(\"=\" * 60)\n",
        "print(f\"Settings: {'Enhanced' if USE_ENHANCED_ADAPTIVE else 'Simple'} | Morph={USE_MORPHOLOGY} | Areaâ‰¥{MIN_AREA} | Probâ‰¥{MIN_MEAN_PROB}\")\n",
        "if USE_ENHANCED_ADAPTIVE:\n",
        "    print(f\"  Alpha={ALPHA_GRAD}, Blur={GAUSSIAN_BLUR_SIZE}, STDÃ—{THRESHOLD_STD_MULT}\")\n",
        "else:\n",
        "    print(f\"  Threshold={SIMPLE_THRESHOLD}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"IoU:       {df['iou'].mean():.4f} Â± {df['iou'].std():.4f}\")\n",
        "print(f\"Dice:      {df['dice'].mean():.4f} Â± {df['dice'].std():.4f}\")\n",
        "print(f\"Precision: {df['precision'].mean():.4f} Â± {df['precision'].std():.4f}\")\n",
        "print(f\"Recall:    {df['recall'].mean():.4f} Â± {df['recall'].std():.4f}\")\n",
        "print(f\"F1:        {df['f1'].mean():.4f} Â± {df['f1'].std():.4f}\")\n",
        "print(f\"Samples with preds: {(df['n_pred'] > 0).sum()}/{len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize samples (worst, random, or specific index)\n",
        "def viz(idx):\n",
        "    item = cache[idx]\n",
        "    probs, size, gt_masks = item[\"probs\"], item[\"size\"], item[\"gt\"]\n",
        "    pred_masks = process_probs(probs, size)\n",
        "    img = dataset[idx][\"image\"].convert(\"RGB\")\n",
        "    \n",
        "    h, w = size[1], size[0]\n",
        "    pred = np.zeros((h, w), dtype=np.uint8)\n",
        "    for m in pred_masks: pred |= m\n",
        "    gt = np.zeros((h, w), dtype=np.uint8)\n",
        "    for m in gt_masks:\n",
        "        if m.shape == (h, w): gt |= m\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    axes[0].imshow(img); axes[0].set_title(\"Image\"); axes[0].axis(\"off\")\n",
        "    axes[1].imshow(gt, cmap=\"Reds\"); axes[1].set_title(f\"GT ({len(gt_masks)})\"); axes[1].axis(\"off\")\n",
        "    axes[2].imshow(pred, cmap=\"Blues\"); axes[2].set_title(f\"Pred ({len(pred_masks)})\"); axes[2].axis(\"off\")\n",
        "    prob_avg = probs.mean(axis=0)\n",
        "    axes[3].imshow(cv2.resize(prob_avg, size), cmap=\"viridis\", vmin=0, vmax=1)\n",
        "    axes[3].set_title(\"Avg Prob\"); axes[3].axis(\"off\")\n",
        "    \n",
        "    iou, dice, _, _, f1 = compute_metrics(pred.astype(bool), gt.astype(bool))\n",
        "    fig.suptitle(f\"Sample {idx} | IoU={iou:.3f} | Dice={dice:.3f} | F1={f1:.3f}\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Show worst, best, and a random sample\n",
        "worst_idx = df.loc[df['f1'].idxmin(), 'idx']\n",
        "best_idx = df.loc[df['f1'].idxmax(), 'idx']\n",
        "print(\"Worst sample:\"); viz(worst_idx)\n",
        "print(\"Best sample:\"); viz(best_idx)\n",
        "print(\"Random sample:\"); viz(random.randint(0, len(cache)-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick parameter sweep (optional - compares different threshold values)\n",
        "def eval_with_params(use_enhanced, threshold=0.5, alpha=0.35, std_mult=0.3, min_area=400, min_prob=0.35):\n",
        "    results = []\n",
        "    for item in cache:\n",
        "        probs, size, gt_masks = item[\"probs\"], item[\"size\"], item[\"gt\"]\n",
        "        masks = []\n",
        "        for ch in range(probs.shape[0]):\n",
        "            p = probs[ch]\n",
        "            if use_enhanced:\n",
        "                gx, gy = cv2.Sobel(p, cv2.CV_32F, 1, 0, ksize=3), cv2.Sobel(p, cv2.CV_32F, 0, 1, ksize=3)\n",
        "                grad = np.sqrt(gx**2 + gy**2) / (np.sqrt(gx**2 + gy**2).max() + 1e-6)\n",
        "                enhanced = cv2.GaussianBlur((1 - alpha) * p + alpha * grad, (3, 3), 0)\n",
        "                mask = (enhanced > np.mean(enhanced) + std_mult * np.std(enhanced)).astype(np.uint8)\n",
        "            else:\n",
        "                mask = (p > threshold).astype(np.uint8)\n",
        "            mask = cv2.resize(mask, size, interpolation=cv2.INTER_NEAREST)\n",
        "            if mask.sum() >= min_area:\n",
        "                prob_r = cv2.resize(p, size, interpolation=cv2.INTER_LINEAR)\n",
        "                if mask.sum() > 0 and prob_r[mask == 1].mean() >= min_prob:\n",
        "                    masks.append(mask)\n",
        "        \n",
        "        h, w = size[1], size[0]\n",
        "        pred = np.zeros((h, w), dtype=bool)\n",
        "        for m in masks: pred |= m.astype(bool)\n",
        "        gt = np.zeros((h, w), dtype=bool)\n",
        "        for m in gt_masks:\n",
        "            if m.shape == (h, w): gt |= m.astype(bool)\n",
        "        \n",
        "        _, _, _, _, f1 = compute_metrics(pred, gt)\n",
        "        results.append(f1)\n",
        "    return np.mean(results)\n",
        "\n",
        "# Compare simple thresholds\n",
        "print(\"Simple threshold sweep:\")\n",
        "for t in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "    f1 = eval_with_params(use_enhanced=False, threshold=t)\n",
        "    print(f\"  threshold={t:.1f} â†’ F1={f1:.4f}\")\n",
        "\n",
        "print(\"\\nEnhanced adaptive sweep (STD mult):\")\n",
        "for s in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    f1 = eval_with_params(use_enhanced=True, std_mult=s)\n",
        "    print(f\"  std_mult={s:.1f} â†’ F1={f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
