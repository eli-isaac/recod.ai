{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, cv2, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TEST_DIR = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\n",
        "SAMPLE_SUB = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\n",
        "WEIGHTS_PATH = \"/kaggle/input/recod-1219/best_model_clean.pt\"  # UPDATE THIS\n",
        "\n",
        "DINO_PATH = \"/kaggle/input/dinov2/pytorch/base/1\"\n",
        "IMG_SIZE = 512\n",
        "CHANNELS = 4\n",
        "UNFREEZE_BLOCKS = 4\n",
        "DECODER_DROPOUT = 0.15\n",
        "\n",
        "# ==================== MODEL ====================\n",
        "class DinoDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=768, out_channels=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.up1 = self._block(in_channels, 384, dropout)\n",
        "        self.up2 = self._block(384, 192, dropout)\n",
        "        self.up3 = self._block(192, 96, dropout)\n",
        "        self.up4 = self._block(96, 48, dropout)\n",
        "        self.final = nn.Conv2d(48, out_channels, kernel_size=1)\n",
        "    \n",
        "    def _block(self, in_ch, out_ch, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(dropout), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
        "        )\n",
        "    \n",
        "    def forward(self, features, target_size):\n",
        "        x = F.interpolate(features, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up1(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up2(x)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up3(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "        x = self.up4(x)\n",
        "        return self.final(x)\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, backbone=\"facebook/dinov2-base\", out_channels=4, unfreeze_blocks=3, decoder_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(backbone)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        self.processor = AutoImageProcessor.from_pretrained(backbone, use_fast=True)\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        num_blocks = len(self.encoder.encoder.layer)\n",
        "        for i in range(num_blocks - unfreeze_blocks, num_blocks):\n",
        "            for param in self.encoder.encoder.layer[i].parameters():\n",
        "                param.requires_grad = True\n",
        "        for param in self.encoder.layernorm.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.decoder = DinoDecoder(hidden_size, out_channels, decoder_dropout)\n",
        "    \n",
        "    def forward_features(self, x):\n",
        "        # Convert from [0, 1] to [0, 255]\n",
        "        imgs = (x * 255).clamp(0, 255).byte().permute(0, 2, 3, 1)\n",
        "        inputs = self.processor(images=imgs, return_tensors=\"pt\")\n",
        "        # Move to same device/dtype as model\n",
        "        inputs = {k: v.to(x.device, x.dtype) if v.is_floating_point() else v.to(x.device) for k, v in inputs.items()}\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B, N, C = feats.shape\n",
        "        fmap = feats[:, 1:, :].permute(0, 2, 1).reshape(B, C, int(math.sqrt(N-1)), int(math.sqrt(N-1)))\n",
        "        return fmap\n",
        "    \n",
        "    def forward(self, x):\n",
        "        target_size = (x.shape[2], x.shape[3])\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.decoder(fmap, target_size)\n",
        "\n",
        "# ==================== LOAD MODEL ====================\n",
        "print(\"Loading model...\")\n",
        "model = DinoSegmenter(DINO_PATH, CHANNELS, UNFREEZE_BLOCKS, DECODER_DROPOUT).to(device)\n",
        "checkpoint = torch.load(WEIGHTS_PATH, map_location=device, weights_only=False)\n",
        "state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "print(\"✅ Model loaded\")\n",
        "\n",
        "# ==================== INFERENCE WITH TTA ====================\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_single(pil):\n",
        "    img = pil.resize((IMG_SIZE, IMG_SIZE))\n",
        "    x = torch.from_numpy(np.array(img, np.float32) / 255.).permute(2, 0, 1)[None].to(device)\n",
        "    return torch.sigmoid(model(x))[0].cpu().numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_tta(pil):\n",
        "    \"\"\"TTA with normal, horizontal flip, vertical flip, and both flips\"\"\"\n",
        "    # Original\n",
        "    probs_orig = segment_prob_map_single(pil)\n",
        "    \n",
        "    # Horizontal flip\n",
        "    pil_hflip = pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    probs_hflip = segment_prob_map_single(pil_hflip)\n",
        "    probs_hflip = probs_hflip[:, :, ::-1]  # flip back\n",
        "    \n",
        "    # Vertical flip\n",
        "    pil_vflip = pil.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "    probs_vflip = segment_prob_map_single(pil_vflip)\n",
        "    probs_vflip = probs_vflip[:, ::-1, :]  # flip back\n",
        "    \n",
        "    # Both flips\n",
        "    pil_both = pil.transpose(Image.FLIP_LEFT_RIGHT).transpose(Image.FLIP_TOP_BOTTOM)\n",
        "    probs_both = segment_prob_map_single(pil_both)\n",
        "    probs_both = probs_both[:, ::-1, ::-1]  # flip back both\n",
        "    \n",
        "    # Average\n",
        "    return (probs_orig + probs_hflip + probs_vflip + probs_both) / 4.0\n",
        "\n",
        "def threshold_mask(prob, thr=0.99):\n",
        "    mask = (prob > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
        "    return mask\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    probs = segment_prob_map_tta(pil)\n",
        "    all_masks = []\n",
        "    for ch in range(probs.shape[0]):\n",
        "        mask = threshold_mask(probs[ch], thr=0.99)\n",
        "        mask = cv2.resize(mask, pil.size, interpolation=cv2.INTER_NEAREST)\n",
        "        area = int(mask.sum())\n",
        "        if area >= 200:\n",
        "            all_masks.append(mask)\n",
        "    return all_masks\n",
        "\n",
        "# ==================== RLE ENCODING ====================\n",
        "def rle_encode_single(mask):\n",
        "    pixels = mask.T.flatten()\n",
        "    dots = np.where(pixels == 1)[0]\n",
        "    if len(dots) == 0:\n",
        "        return None\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return json.dumps([int(x) for x in run_lengths])\n",
        "\n",
        "def rle_encode_multi(masks):\n",
        "    encoded = [rle_encode_single((m > 0).astype(np.uint8)) for m in masks]\n",
        "    encoded = [e for e in encoded if e is not None]\n",
        "    return ';'.join(encoded) if encoded else \"authentic\"\n",
        "\n",
        "# ==================== GENERATE SUBMISSION ====================\n",
        "rows = []\n",
        "test_files = sorted(os.listdir(TEST_DIR))\n",
        "print(f\"Processing {len(test_files)} test images...\")\n",
        "\n",
        "for f in test_files:\n",
        "    pil = Image.open(Path(TEST_DIR) / f).convert(\"RGB\")\n",
        "    masks = pipeline_final(pil)\n",
        "    annot = rle_encode_multi(masks) if masks else \"authentic\"\n",
        "    rows.append({\"case_id\": Path(f).stem, \"annotation\": annot})\n",
        "\n",
        "sub = pd.DataFrame(rows)\n",
        "ss = pd.read_csv(SAMPLE_SUB)\n",
        "ss[\"case_id\"] = ss[\"case_id\"].astype(str)\n",
        "sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n",
        "final = ss[[\"case_id\"]].merge(sub, on=\"case_id\", how=\"left\")\n",
        "final[\"annotation\"] = final[\"annotation\"].fillna(\"authentic\")\n",
        "final[[\"case_id\", \"annotation\"]].to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "import os\n",
        "print(f\"\\n✅ Saved submission.csv\")\n",
        "print(f\"File exists: {os.path.exists('submission.csv')}\")\n",
        "print(f\"Total: {len(final)} | Forged: {(final['annotation'] != 'authentic').sum()} | Authentic: {(final['annotation'] == 'authentic').sum()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
