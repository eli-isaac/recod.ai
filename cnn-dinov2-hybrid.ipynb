{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f370112b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/cali-recod/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e1c76a01",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T23:52:45.347328Z",
          "iopub.status.busy": "2025-11-10T23:52:45.347097Z",
          "iopub.status.idle": "2025-11-11T01:39:56.040723Z",
          "shell.execute_reply": "2025-11-11T01:39:56.039613Z"
        },
        "papermill": {
          "duration": 6430.697833,
          "end_time": "2025-11-11T01:39:56.041865",
          "exception": false,
          "start_time": "2025-11-10T23:52:45.344032",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "BASE_DIR  = \"./data\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "DINO_PATH = \"facebook/dinov2-base\"\n",
        "\n",
        "IMG_SIZE = 512\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_SEG = 10\n",
        "LR_SEG = 1e-4\n",
        "WEIGHT_DECAY = 2e-5\n",
        "CHANNELS = 4\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=256, transform=False):\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m))\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None))\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        # Define transforms once in __init__, not in __getitem__\n",
        "        if self.transform:\n",
        "            self.aug = A.Compose([\n",
        "                A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "                # Geometric augmentations\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.RandomRotate90(p=0.5),\n",
        "                # Color augmentations (important for generalization)\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
        "                # Noise/blur (helps with real-world robustness)\n",
        "                A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
        "                A.GaussNoise(var_limit=(10, 50), p=0.2),\n",
        "                # JPEG compression artifacts (very relevant for forgery detection!)\n",
        "                A.ImageCompression(quality_lower=70, quality_upper=100, p=0.3),\n",
        "                # Convert to tensor (no normalization - processor handles it)\n",
        "                ToTensorV2(),\n",
        "            ])\n",
        "        else:\n",
        "            self.aug = A.Compose([\n",
        "                A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "                # Convert to tensor (no normalization - processor handles it)\n",
        "                ToTensorV2(),\n",
        "            ])\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")  # PIL Image (always RGB regardless of source format)\n",
        "        img = np.array(img)  # numpy (H, W, C) - albumentations expects channels last\n",
        "        h, w = img.shape[:2]\n",
        "        \n",
        "        masks = np.zeros((h, w, CHANNELS), np.uint8)  # (H, W, C) for albumentations\n",
        "        if mask_path:\n",
        "            loaded_masks = np.load(mask_path)  # Shape: (num_masks, H, W)\n",
        "            len_masks = len(loaded_masks)\n",
        "            # Transpose to (H, W, C)\n",
        "            for i in range(min(len_masks, CHANNELS)):\n",
        "                masks[:, :, i] = loaded_masks[i]\n",
        "        \n",
        "        # Apply transforms\n",
        "        transformed = self.aug(image=img, mask=masks)\n",
        "        img_t = transformed['image']  # Already a tensor with shape (C, H, W), totensorv2 does this\n",
        "        masks_t = transformed['mask']  # Shape: (H, W, C), totensorv2 does not permute\n",
        "        \n",
        "        # Scale image to [0, 1] range (ToTensorV2 keeps [0, 255])\n",
        "        # forward_features() expects [0, 1] and converts back to [0, 255] for the processor\n",
        "        img_t = img_t.float() / 255.0\n",
        "        \n",
        "        # Transpose mask back to (C, H, W)\n",
        "        if isinstance(masks_t, np.ndarray):\n",
        "            masks_t = torch.from_numpy(masks_t.astype(np.float32)).permute(2, 0, 1)\n",
        "        else:\n",
        "            masks_t = masks_t.permute(2, 0, 1).float()\n",
        "        \n",
        "        # they are now both C,H,W\n",
        "        return img_t, masks_t\n",
        "\n",
        "\n",
        "class DinoDecoder(nn.Module):\n",
        "    \"\"\"Progressive upsampling decoder with regularization\"\"\"\n",
        "    def __init__(self, in_ch=768, out_ch=CHANNELS, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Conv blocks for each upsampling stage (upsampling done in forward with F.interpolate)\n",
        "        self.up1 = self._block(in_ch, 384, dropout)    # 768 → 384 channels\n",
        "        self.up2 = self._block(384, 192, dropout)      # 384 → 192 channels\n",
        "        self.up3 = self._block(192, 96, dropout)       # 192 → 96 channels\n",
        "        self.up4 = self._block(96, 48, dropout)        # 96 → 48 channels\n",
        "        \n",
        "        self.final = nn.Conv2d(48, out_ch, kernel_size=1)\n",
        "    \n",
        "    def _block(self, in_ch, out_ch, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(dropout),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    \n",
        "    def forward(self, f, size):\n",
        "        # Progressive upsampling: 37×37 → 74 → 148 → 296 → 512\n",
        "        x = F.interpolate(f, scale_factor=2, mode='bilinear', align_corners=False)  # 37 → 74\n",
        "        x = self.up1(x)\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)  # 74 → 148\n",
        "        x = self.up2(x)\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)  # 148 → 296\n",
        "        x = self.up3(x)\n",
        "        \n",
        "        x = F.interpolate(x, size=size, mode='bilinear', align_corners=False)       # 296 → 512\n",
        "        x = self.up4(x)\n",
        "        \n",
        "        return self.final(x)\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor, unfreeze_blocks=3):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        \n",
        "        # Freeze all parameters first\n",
        "        for p in self.encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        # Unfreeze the last N transformer blocks\n",
        "        num_blocks = len(self.encoder.encoder.layer)\n",
        "        for i in range(num_blocks - unfreeze_blocks, num_blocks):\n",
        "            for p in self.encoder.encoder.layer[i].parameters():\n",
        "                p.requires_grad = True\n",
        "        \n",
        "        # Unfreeze the final layernorm\n",
        "        for p in self.encoder.layernorm.parameters():\n",
        "            p.requires_grad = True\n",
        "        \n",
        "        self.seg_head = DinoDecoder(768, CHANNELS)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        # Remove torch.no_grad() since we're fine-tuning!\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B, N, C = feats.shape\n",
        "        fmap = feats[:, 1:, :].permute(0, 2, 1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B, C, s, s)\n",
        "        return fmap\n",
        "\n",
        "    def forward_seg(self, x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap, (IMG_SIZE, IMG_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "723ba7d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Loading weights: 100%|██████████| 223/223 [00:00<00:00, 716.17it/s, Materializing param=layernorm.weight]                                 \n"
          ]
        }
      ],
      "source": [
        "#  TRAINING\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH).eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e669b4db",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3924/2700101391.py:40: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10, 50), p=0.2),\n",
            "/tmp/ipykernel_3924/2700101391.py:42: UserWarning: Argument(s) 'quality_lower, quality_upper' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=70, quality_upper=100, p=0.3),\n"
          ]
        }
      ],
      "source": [
        "auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ForgerySegDataset(train_auth, train_forg, MASK_DIR, transform=True),  # <-- Enable augmentations!\n",
        "    batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    ForgerySegDataset(val_auth, val_forg, MASK_DIR, transform=False),  # <-- No augmentations for validation\n",
        "    batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e1dd7372",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "# load weights\n",
        "# model_seg.load_state_dict(torch.load(\"model_seg_final.pt\"))\n",
        "\n",
        "# opt_seg = optim.AdamW(model_seg.seg_head.parameters(), lr=LR_SEG, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Separate parameter groups with different learning rates\n",
        "backbone_params = []\n",
        "decoder_params = []\n",
        "\n",
        "for name, param in model_seg.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        if 'seg_head' in name:\n",
        "            decoder_params.append(param)\n",
        "        else:\n",
        "            backbone_params.append(param)\n",
        "\n",
        "opt_seg = optim.AdamW([\n",
        "    {'params': backbone_params, 'lr': LR_SEG / 10},  # 10x smaller LR for backbone\n",
        "    {'params': decoder_params, 'lr': LR_SEG},\n",
        "], weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "pos_weight = torch.tensor([99.0]).to(device)  # Adjust this value\n",
        "crit_seg = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# crit_seg = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# baseline\n",
        "# val_loss = .44, val_f1 = .17\n",
        "# augmented data: .1783 (10 epochs)\n",
        "# better model structure: val f1 = .23\n",
        "# hungarian matching: val f1 = 0.2645 (15 epochs, overfits with more epochs), reached 0.3100 with more epochs\n",
        "train_losses = []\n",
        "val_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b90d8ee6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def hungarian_matched_loss(outputs, targets, pos_weight=None):\n",
        "    \"\"\"\n",
        "    Compute BCE loss with Hungarian matching between pred/gt channels.\n",
        "    \n",
        "    outputs: (B, C, H, W) - predicted logits\n",
        "    targets: (B, C, H, W) - ground truth masks\n",
        "    \"\"\"\n",
        "    B, C, H, W = outputs.shape\n",
        "    device = outputs.device\n",
        "    \n",
        "    matched_losses = []\n",
        "    \n",
        "    for b in range(B):\n",
        "        pred = outputs[b]  # (C, H, W)\n",
        "        tgt = targets[b]   # (C, H, W)\n",
        "        \n",
        "        # Build cost matrix: cost[i,j] = BCE(pred_channel_i, gt_channel_j)\n",
        "        with torch.no_grad():\n",
        "            cost_matrix = torch.zeros(C, C)\n",
        "            for i in range(C):\n",
        "                for j in range(C):\n",
        "                    cost_matrix[i, j] = F.binary_cross_entropy_with_logits(\n",
        "                        pred[i], tgt[j], \n",
        "                        pos_weight=pos_weight,\n",
        "                        reduction='mean'\n",
        "                    )\n",
        "        \n",
        "        # Hungarian matching (find optimal assignment)\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_matrix.cpu().numpy())\n",
        "        \n",
        "        # Compute loss for matched pairs (WITH gradients this time)\n",
        "        for r, c in zip(row_ind, col_ind):\n",
        "            loss = F.binary_cross_entropy_with_logits(\n",
        "                pred[r], tgt[c],\n",
        "                pos_weight=pos_weight,\n",
        "                reduction='mean'\n",
        "            )\n",
        "            matched_losses.append(loss)\n",
        "    \n",
        "    return torch.stack(matched_losses).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3e515117",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot train and val losses\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "74a43da7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 1/15: 100%|██████████| 129/129 [02:27<00:00,  1.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.8480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:38<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.7609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 2/15: 100%|██████████| 129/129 [01:55<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.7056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:25<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.6566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 3/15: 100%|██████████| 129/129 [01:51<00:00,  1.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.6290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:27<00:00,  1.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.5828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 4/15: 100%|██████████| 129/129 [01:57<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.5662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:26<00:00,  1.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.5379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 5/15: 100%|██████████| 129/129 [01:55<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.5151\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:25<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.5220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 6/15: 100%|██████████| 129/129 [01:52<00:00,  1.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.4708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:28<00:00,  1.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.4461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 7/15: 100%|██████████| 129/129 [02:01<00:00,  1.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.4428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:27<00:00,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.4221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 8/15: 100%|██████████| 129/129 [02:03<00:00,  1.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.4274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:25<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.4017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 9/15: 100%|██████████| 129/129 [01:56<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.4002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:25<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 10/15: 100%|██████████| 129/129 [01:56<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3760\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:26<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 11/15: 100%|██████████| 129/129 [01:56<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:24<00:00,  1.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3626\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 12/15: 100%|██████████| 129/129 [01:58<00:00,  1.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:26<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 13/15: 100%|██████████| 129/129 [01:50<00:00,  1.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:24<00:00,  1.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 14/15: 100%|██████████| 129/129 [01:58<00:00,  1.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:26<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Segmentation] Epoch 15/15: 100%|██████████| 129/129 [01:55<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 33/33 [00:29<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → avg_loss=0.3376\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_SEG = 15\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for e in range(EPOCHS_SEG):\n",
        "    model_seg.train()\n",
        "    total_train_loss = 0\n",
        "    for x, m in tqdm(train_loader, desc=f\"[Segmentation] Epoch {e+1}/{EPOCHS_SEG}\"):\n",
        "        x, m = x.to(device), m.to(device)\n",
        "        outputs = model_seg.forward_seg(x)\n",
        "        loss = hungarian_matched_loss(outputs, m, pos_weight=pos_weight)\n",
        "        opt_seg.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_seg.step()\n",
        "        total_train_loss += loss.item()\n",
        "    print(f\"  → train_avg_loss={total_train_loss/len(train_loader):.4f}\")\n",
        "    train_losses.append(total_train_loss/len(train_loader))\n",
        "\n",
        "    # compute val loss\n",
        "    model_seg.eval()\n",
        "    total_val_loss = 0\n",
        "    for x, m in tqdm(val_loader, desc=\"Validation\"):\n",
        "        x, m = x.to(device), m.to(device)\n",
        "        outputs = model_seg.forward_seg(x)\n",
        "        loss = hungarian_matched_loss(outputs, m, pos_weight=pos_weight)\n",
        "        total_val_loss += loss.item()\n",
        "    \n",
        "    avg_val_loss = total_val_loss/len(val_loader)\n",
        "    print(f\"  → val_avg_loss={avg_val_loss:.4f}\")\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        print(f\"  ★ New best val loss: {best_val_loss:.4f}. Saving model...\")\n",
        "        torch.save(model_seg.state_dict(), \"model_seg_best.pt\")\n",
        "\n",
        "torch.save(model_seg.state_dict(), \"model_seg_final.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "56fa3c53",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def segment_prob_map_all_channels(pil):\n",
        "    \"\"\"Returns probability maps for ALL channels (not just channel 0).\"\"\"\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    return torch.sigmoid(model_seg.forward_seg(x))[0].cpu().numpy()  # Shape: (CHANNELS, H, W)\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.35):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    \"\"\"Returns a LIST of masks (one per detected forged region) instead of a single merged mask.\"\"\"\n",
        "    probs = segment_prob_map_all_channels(pil)  # (CHANNELS, H, W)\n",
        "    \n",
        "    all_masks = []\n",
        "    all_areas = []\n",
        "    all_means = []\n",
        "    all_thrs = []\n",
        "    \n",
        "    for ch in range(probs.shape[0]):\n",
        "        prob = probs[ch]\n",
        "        mask, thr = finalize_mask(prob, pil.size)\n",
        "        area = int(mask.sum())\n",
        "        \n",
        "        if area > 0:\n",
        "            prob_resized = cv2.resize(prob, pil.size, interpolation=cv2.INTER_LINEAR)\n",
        "            mean_inside = float(prob_resized[mask == 1].mean())\n",
        "        else:\n",
        "            mean_inside = 0.0\n",
        "        \n",
        "        # Filter out small/weak detections\n",
        "        if area >= 400 and mean_inside >= 0.35:\n",
        "            all_masks.append(mask)\n",
        "            all_areas.append(area)\n",
        "            all_means.append(mean_inside)\n",
        "            all_thrs.append(thr)\n",
        "    \n",
        "    if len(all_masks) == 0:\n",
        "        # No valid masks found\n",
        "        return \"authentic\", [], {\"area\": 0, \"mean_inside\": 0.0, \"thr\": 0.0}\n",
        "    \n",
        "    total_area = sum(all_areas)\n",
        "    avg_mean = sum(all_means) / len(all_means)\n",
        "    avg_thr = sum(all_thrs) / len(all_thrs)\n",
        "    \n",
        "    return \"forged\", all_masks, {\"area\": total_area, \"mean_inside\": avg_mean, \"thr\": avg_thr, \"num_masks\": len(all_masks)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d0b3566a",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def segment_prob_map_all_channels(pil):\n",
        "    \"\"\"Returns probability maps for ALL channels (not just channel 0).\"\"\"\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    return torch.sigmoid(model_seg.forward_seg(x))[0].cpu().numpy()  # Shape: (CHANNELS, H, W)\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.35):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    \"\"\"Returns a LIST of masks (one per detected forged region) instead of a single merged mask.\"\"\"\n",
        "    probs = segment_prob_map_all_channels(pil)  # (CHANNELS, H, W)\n",
        "    \n",
        "    all_masks = []\n",
        "    all_areas = []\n",
        "    all_means = []\n",
        "    all_thrs = []\n",
        "    \n",
        "    for ch in range(probs.shape[0]):\n",
        "        prob = probs[ch]\n",
        "        mask, thr = finalize_mask(prob, pil.size)\n",
        "        area = int(mask.sum())\n",
        "        \n",
        "        if area > 0:\n",
        "            prob_resized = cv2.resize(prob, pil.size, interpolation=cv2.INTER_LINEAR)\n",
        "            mean_inside = float(prob_resized[mask == 1].mean())\n",
        "        else:\n",
        "            mean_inside = 0.0\n",
        "        \n",
        "        # Filter out small/weak detections\n",
        "        if area >= 400 and mean_inside >= 0.35:\n",
        "            all_masks.append(mask)\n",
        "            all_areas.append(area)\n",
        "            all_means.append(mean_inside)\n",
        "            all_thrs.append(thr)\n",
        "    \n",
        "    if len(all_masks) == 0:\n",
        "        # No valid masks found\n",
        "        return \"authentic\", [], {\"area\": 0, \"mean_inside\": 0.0, \"thr\": 0.0}\n",
        "    \n",
        "    total_area = sum(all_areas)\n",
        "    avg_mean = sum(all_means) / len(all_means)\n",
        "    avg_thr = sum(all_thrs) / len(all_thrs)\n",
        "    \n",
        "    return \"forged\", all_masks, {\"area\": total_area, \"mean_inside\": avg_mean, \"thr\": avg_thr, \"num_masks\": len(all_masks)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4a9fc331",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import numba\n",
        "import numpy as np\n",
        "from numba import types\n",
        "import numpy.typing as npt\n",
        "import pandas as pd\n",
        "import scipy.optimize\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "@numba.jit(nopython=True)\n",
        "def _rle_encode_jit(x: npt.NDArray, fg_val: int = 1) -> list[int]:\n",
        "    \"\"\"Numba-jitted RLE encoder.\"\"\"\n",
        "    dots = np.where(x.T.flatten() == fg_val)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "\n",
        "def rle_encode(masks: list[npt.NDArray], fg_val: int = 1) -> str:\n",
        "    \"\"\"\n",
        "    Adapted from contrails RLE https://www.kaggle.com/code/inversion/contrails-rle-submission\n",
        "    Args:\n",
        "        masks: list of numpy array of shape (height, width), 1 - mask, 0 - background\n",
        "    Returns: run length encodings as a string, with each RLE JSON-encoded and separated by a semicolon.\n",
        "    \"\"\"\n",
        "    return ';'.join([json.dumps(_rle_encode_jit(x, fg_val)) for x in masks])\n",
        "\n",
        "\n",
        "@numba.njit\n",
        "def _rle_decode_jit(mask_rle: npt.NDArray, height: int, width: int) -> npt.NDArray:\n",
        "    \"\"\"\n",
        "    s: numpy array of run-length encoding pairs (start, length)\n",
        "    shape: (height, width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    \"\"\"\n",
        "    if len(mask_rle) % 2 != 0:\n",
        "        # Numba requires raising a standard exception.\n",
        "        raise ValueError('One or more rows has an odd number of values.')\n",
        "\n",
        "    starts, lengths = mask_rle[0::2], mask_rle[1::2]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    for i in range(len(starts) - 1):\n",
        "        if ends[i] > starts[i + 1]:\n",
        "            raise ValueError('Pixels must not be overlapping.')\n",
        "    img = np.zeros(height * width, dtype=np.bool_)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img\n",
        "\n",
        "\n",
        "def rle_decode(mask_rle: str, shape: tuple[int, int]) -> npt.NDArray:\n",
        "    \"\"\"\n",
        "    mask_rle: run-length as string formatted (start length)\n",
        "              empty predictions need to be encoded with '-'\n",
        "    shape: (height, width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    \"\"\"\n",
        "\n",
        "    mask_rle = json.loads(mask_rle)\n",
        "    mask_rle = np.asarray(mask_rle, dtype=np.int32)\n",
        "    starts = mask_rle[0::2]\n",
        "    if sorted(starts) != list(starts):\n",
        "        raise ParticipantVisibleError('Submitted values must be in ascending order.')\n",
        "    try:\n",
        "        return _rle_decode_jit(mask_rle, shape[0], shape[1]).reshape(shape, order='F')\n",
        "    except ValueError as e:\n",
        "        raise ParticipantVisibleError(str(e)) from e\n",
        "\n",
        "\n",
        "# ===================== GPU-ACCELERATED SCORING FUNCTIONS =====================\n",
        "\n",
        "def calculate_f1_matrix_gpu(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray], device='cuda'):\n",
        "    \"\"\"\n",
        "    GPU-accelerated F1 matrix calculation.\n",
        "    Computes F1 scores for all pairs of predicted and ground truth masks in parallel.\n",
        "    \n",
        "    Parameters:\n",
        "    pred_masks: List of predicted binary masks (each is height x width)\n",
        "    gt_masks: List of ground truth binary masks (each is height x width)\n",
        "    device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    numpy array of shape (num_pred, num_gt) containing F1 scores\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        device = 'cpu'\n",
        "    \n",
        "    num_pred = len(pred_masks)\n",
        "    num_gt = len(gt_masks)\n",
        "    \n",
        "    if num_pred == 0 or num_gt == 0:\n",
        "        return np.zeros((max(num_pred, num_gt), num_gt))\n",
        "    \n",
        "    # Stack and flatten masks, convert to GPU tensors\n",
        "    # Shape: (num_pred, H*W) and (num_gt, H*W)\n",
        "    pred_flat = torch.stack([torch.from_numpy(m.flatten().astype(np.float32)) for m in pred_masks]).to(device)\n",
        "    gt_flat = torch.stack([torch.from_numpy(m.flatten().astype(np.float32)) for m in gt_masks]).to(device)\n",
        "    \n",
        "    # Expand dims for broadcasting: pred (num_pred, 1, H*W), gt (1, num_gt, H*W)\n",
        "    pred_expanded = pred_flat.unsqueeze(1)  # (num_pred, 1, H*W)\n",
        "    gt_expanded = gt_flat.unsqueeze(0)       # (1, num_gt, H*W)\n",
        "    \n",
        "    # Compute TP, FP, FN for all pairs at once using broadcasting\n",
        "    # This creates (num_pred, num_gt, H*W) intermediate tensors\n",
        "    tp = torch.sum((pred_expanded == 1) & (gt_expanded == 1), dim=2).float()  # (num_pred, num_gt)\n",
        "    fp = torch.sum((pred_expanded == 1) & (gt_expanded == 0), dim=2).float()\n",
        "    fn = torch.sum((pred_expanded == 0) & (gt_expanded == 1), dim=2).float()\n",
        "    \n",
        "    # Compute precision and recall\n",
        "    precision = tp / (tp + fp + 1e-10)\n",
        "    recall = tp / (tp + fn + 1e-10)\n",
        "    \n",
        "    # Handle edge cases where tp + fp or tp + fn is 0\n",
        "    precision = torch.where((tp + fp) > 0, precision, torch.zeros_like(precision))\n",
        "    recall = torch.where((tp + fn) > 0, recall, torch.zeros_like(recall))\n",
        "    \n",
        "    # Compute F1\n",
        "    f1_matrix = 2 * precision * recall / (precision + recall + 1e-10)\n",
        "    f1_matrix = torch.where((precision + recall) > 0, f1_matrix, torch.zeros_like(f1_matrix))\n",
        "    \n",
        "    f1_matrix = f1_matrix.cpu().numpy()\n",
        "    \n",
        "    # Pad if needed\n",
        "    if f1_matrix.shape[0] < num_gt:\n",
        "        f1_matrix = np.vstack((f1_matrix, np.zeros((num_gt - f1_matrix.shape[0], num_gt))))\n",
        "    \n",
        "    return f1_matrix\n",
        "\n",
        "\n",
        "def calculate_f1_matrix_gpu_batched(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray], \n",
        "                                     device='cuda', batch_size=50):\n",
        "    \"\"\"\n",
        "    Memory-efficient GPU F1 matrix calculation with batching.\n",
        "    Use this for large numbers of masks to avoid GPU OOM.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        device = 'cpu'\n",
        "    \n",
        "    num_pred = len(pred_masks)\n",
        "    num_gt = len(gt_masks)\n",
        "    \n",
        "    if num_pred == 0 or num_gt == 0:\n",
        "        return np.zeros((max(num_pred, num_gt), num_gt))\n",
        "    \n",
        "    # Pre-convert all masks to tensors on GPU\n",
        "    pred_tensors = [torch.from_numpy(m.flatten().astype(np.float32)).to(device) for m in pred_masks]\n",
        "    gt_tensors = [torch.from_numpy(m.flatten().astype(np.float32)).to(device) for m in gt_masks]\n",
        "    \n",
        "    f1_matrix = np.zeros((num_pred, num_gt), dtype=np.float32)\n",
        "    \n",
        "    # Process in batches to avoid OOM\n",
        "    for i_start in range(0, num_pred, batch_size):\n",
        "        i_end = min(i_start + batch_size, num_pred)\n",
        "        pred_batch = torch.stack(pred_tensors[i_start:i_end])  # (batch, H*W)\n",
        "        \n",
        "        for j_start in range(0, num_gt, batch_size):\n",
        "            j_end = min(j_start + batch_size, num_gt)\n",
        "            gt_batch = torch.stack(gt_tensors[j_start:j_end])  # (batch, H*W)\n",
        "            \n",
        "            # Broadcast and compute\n",
        "            pred_exp = pred_batch.unsqueeze(1)  # (batch_pred, 1, H*W)\n",
        "            gt_exp = gt_batch.unsqueeze(0)       # (1, batch_gt, H*W)\n",
        "            \n",
        "            tp = torch.sum((pred_exp == 1) & (gt_exp == 1), dim=2).float()\n",
        "            fp = torch.sum((pred_exp == 1) & (gt_exp == 0), dim=2).float()\n",
        "            fn = torch.sum((pred_exp == 0) & (gt_exp == 1), dim=2).float()\n",
        "            \n",
        "            precision = torch.where((tp + fp) > 0, tp / (tp + fp), torch.zeros_like(tp))\n",
        "            recall = torch.where((tp + fn) > 0, tp / (tp + fn), torch.zeros_like(tp))\n",
        "            \n",
        "            f1 = torch.where((precision + recall) > 0, \n",
        "                            2 * precision * recall / (precision + recall), \n",
        "                            torch.zeros_like(precision))\n",
        "            \n",
        "            f1_matrix[i_start:i_end, j_start:j_end] = f1.cpu().numpy()\n",
        "    \n",
        "    # Pad if needed\n",
        "    if f1_matrix.shape[0] < num_gt:\n",
        "        f1_matrix = np.vstack((f1_matrix, np.zeros((num_gt - f1_matrix.shape[0], num_gt))))\n",
        "    \n",
        "    return f1_matrix\n",
        "\n",
        "\n",
        "def oF1_score_gpu(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray], device='cuda'):\n",
        "    \"\"\"\n",
        "    GPU-accelerated optimal F1 score calculation.\n",
        "    Uses GPU for F1 matrix computation, CPU for Hungarian algorithm.\n",
        "    \"\"\"\n",
        "    # Use batched version for memory efficiency\n",
        "    f1_matrix = calculate_f1_matrix_gpu_batched(pred_masks, gt_masks, device=device)\n",
        "    \n",
        "    # Hungarian algorithm runs on CPU (scipy)\n",
        "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-f1_matrix)\n",
        "    excess_predictions_penalty = len(gt_masks) / max(len(pred_masks), len(gt_masks))\n",
        "    return np.mean(f1_matrix[row_ind, col_ind]) * excess_predictions_penalty\n",
        "\n",
        "\n",
        "def evaluate_single_image_gpu(label_rles: str, prediction_rles: str, shape_str: str, device='cuda') -> float:\n",
        "    \"\"\"GPU-accelerated single image evaluation.\"\"\"\n",
        "    shape = json.loads(shape_str)\n",
        "    label_masks = [rle_decode(x, shape=shape) for x in label_rles.split(';')]\n",
        "    prediction_masks = [rle_decode(x, shape=shape) for x in prediction_rles.split(';')]\n",
        "    return oF1_score_gpu(prediction_masks, label_masks, device=device)\n",
        "\n",
        "\n",
        "def score_gpu(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, \n",
        "              device='cuda', num_workers: int = 1, return_details: bool = True):\n",
        "    \"\"\"\n",
        "    GPU-accelerated scoring function with parallel processing.\n",
        "    \n",
        "    Args:\n",
        "        solution: Ground truth DataFrame\n",
        "        submission: Submission DataFrame  \n",
        "        row_id_column_name: Column name for row IDs\n",
        "        device: 'cuda' or 'cpu'\n",
        "        num_workers: Number of parallel workers for batch processing\n",
        "        return_details: If True, returns (score, details_df) tuple\n",
        "        \n",
        "    Returns:\n",
        "        If return_details=True: (float, pd.DataFrame) - score and detailed results per row\n",
        "        If return_details=False: float - just the score\n",
        "    \"\"\"\n",
        "    import gc\n",
        "    import traceback\n",
        "    \n",
        "    df = solution.copy()\n",
        "    df = df.rename(columns={'annotation': 'label'})\n",
        "    df['prediction'] = submission['annotation']\n",
        "    \n",
        "    # Initialize result columns\n",
        "    df['image_score'] = 0.0\n",
        "    df['success'] = True\n",
        "    df['error_msg'] = ''\n",
        "    df['eval_type'] = 'pending'\n",
        "    df['num_pred_masks'] = 0\n",
        "    df['num_gt_masks'] = 0\n",
        "    df['shape_parsed'] = ''\n",
        "    # Keep existing 'image' column if present, otherwise initialize empty\n",
        "    if 'image' not in df.columns:\n",
        "        df['image'] = ''\n",
        "    \n",
        "    # Identify authentic vs forged\n",
        "    authentic_indices = (df['label'] == 'authentic') | (df['prediction'] == 'authentic')\n",
        "    \n",
        "    # Handle authentic cases\n",
        "    df.loc[authentic_indices, 'image_score'] = ((df['label'] == df['prediction']) & authentic_indices).astype(float)\n",
        "    df.loc[authentic_indices, 'eval_type'] = 'authentic'\n",
        "    \n",
        "    # Get rows that need evaluation\n",
        "    eval_indices = df.loc[~authentic_indices].index.tolist()\n",
        "    \n",
        "    if len(eval_indices) == 0:\n",
        "        final_score = float(np.mean(df['image_score']))\n",
        "        if return_details:\n",
        "            return final_score, df\n",
        "        return final_score\n",
        "    \n",
        "    print(f\"Evaluating {len(eval_indices)} forged images on GPU...\")\n",
        "    \n",
        "    # Process sequentially to better handle memory and catch errors per-row\n",
        "    for idx in tqdm(eval_indices, desc=\"GPU Scoring\"):\n",
        "        row = df.loc[idx]\n",
        "        try:\n",
        "            # Parse shape for debugging\n",
        "            shape = json.loads(row['shape'])\n",
        "            df.loc[idx, 'shape_parsed'] = str(shape)\n",
        "            df.loc[idx, 'image'] = row['image']\n",
        "            \n",
        "            # Count masks\n",
        "            label_rles = row['label'].split(';')\n",
        "            pred_rles = row['prediction'].split(';')\n",
        "            df.loc[idx, 'num_gt_masks'] = len(label_rles)\n",
        "            df.loc[idx, 'num_pred_masks'] = len(pred_rles)\n",
        "            \n",
        "            # Evaluate\n",
        "            result = evaluate_single_image_gpu(row['label'], row['prediction'], row['shape'], device=device)\n",
        "            df.loc[idx, 'image_score'] = result\n",
        "            df.loc[idx, 'eval_type'] = 'forged_evaluated'\n",
        "            df.loc[idx, 'success'] = True\n",
        "            \n",
        "        except torch.cuda.OutOfMemoryError as e:\n",
        "            df.loc[idx, 'success'] = False\n",
        "            df.loc[idx, 'error_msg'] = f\"CUDA OOM: {str(e)}\"\n",
        "            df.loc[idx, 'eval_type'] = 'error_oom'\n",
        "            df.loc[idx, 'image_score'] = 0.0\n",
        "            print(f\"\\n⚠️ CUDA OOM at index {idx}: {str(e)[:100]}\")\n",
        "            # Try to recover\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "        except Exception as e:\n",
        "            df.loc[idx, 'success'] = False\n",
        "            df.loc[idx, 'error_msg'] = f\"{type(e).__name__}: {str(e)}\"\n",
        "            df.loc[idx, 'eval_type'] = 'error_other'\n",
        "            df.loc[idx, 'image_score'] = 0.0\n",
        "            print(f\"\\n⚠️ Error at index {idx}: {type(e).__name__}: {str(e)[:100]}\")\n",
        "            \n",
        "        # Periodically clear GPU cache\n",
        "        if idx % 50 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "    \n",
        "    final_score = float(np.mean(df['image_score']))\n",
        "    \n",
        "    # Print summary\n",
        "    num_success = df['success'].sum()\n",
        "    num_failed = (~df['success']).sum()\n",
        "    print(f\"\\n✅ Completed: {num_success} succeeded, {num_failed} failed\")\n",
        "    print(f\"📊 Final score: {final_score:.4f}\")\n",
        "    \n",
        "    if return_details:\n",
        "        return final_score, df\n",
        "    return final_score\n",
        "\n",
        "\n",
        "# ===================== LEGACY CPU FUNCTIONS (kept for compatibility) =====================\n",
        "\n",
        "def calculate_f1_score(pred_mask: npt.NDArray, gt_mask: npt.NDArray):\n",
        "    pred_flat = pred_mask.flatten()\n",
        "    gt_flat = gt_mask.flatten()\n",
        "\n",
        "    tp = np.sum((pred_flat == 1) & (gt_flat == 1))\n",
        "    fp = np.sum((pred_flat == 1) & (gt_flat == 0))\n",
        "    fn = np.sum((pred_flat == 0) & (gt_flat == 1))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    if (precision + recall) > 0:\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def calculate_f1_matrix(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n",
        "    \"\"\"CPU version - use calculate_f1_matrix_gpu for faster processing.\"\"\"\n",
        "    num_instances_pred = len(pred_masks)\n",
        "    num_instances_gt = len(gt_masks)\n",
        "    f1_matrix = np.zeros((num_instances_pred, num_instances_gt))\n",
        "\n",
        "    for i in range(num_instances_pred):\n",
        "        for j in range(num_instances_gt):\n",
        "            pred_flat = pred_masks[i].flatten()\n",
        "            gt_flat = gt_masks[j].flatten()\n",
        "            f1_matrix[i, j] = calculate_f1_score(pred_mask=pred_flat, gt_mask=gt_flat)\n",
        "\n",
        "    if f1_matrix.shape[0] < len(gt_masks):\n",
        "        f1_matrix = np.vstack((f1_matrix, np.zeros((len(gt_masks) - len(f1_matrix), num_instances_gt))))\n",
        "\n",
        "    return f1_matrix\n",
        "\n",
        "\n",
        "def oF1_score(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n",
        "    \"\"\"CPU version - use oF1_score_gpu for faster processing.\"\"\"\n",
        "    f1_matrix = calculate_f1_matrix(pred_masks, gt_masks)\n",
        "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-f1_matrix)\n",
        "    excess_predictions_penalty = len(gt_masks) / max(len(pred_masks), len(gt_masks))\n",
        "    return np.mean(f1_matrix[row_ind, col_ind]) * excess_predictions_penalty\n",
        "\n",
        "\n",
        "def evaluate_single_image(label_rles: str, prediction_rles: str, shape_str: str) -> float:\n",
        "    shape = json.loads(shape_str)\n",
        "    label_rles = [rle_decode(x, shape=shape) for x in label_rles.split(';')]\n",
        "    prediction_rles = [rle_decode(x, shape=shape) for x in prediction_rles.split(';')]\n",
        "    return oF1_score(prediction_rles, label_rles)\n",
        "\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
        "    \"\"\"CPU version - use score_gpu for faster processing.\"\"\"\n",
        "    df = solution\n",
        "    df = df.rename(columns={'annotation': 'label'})\n",
        "\n",
        "    df['prediction'] = submission['annotation']\n",
        "    authentic_indices = (df['label'] == 'authentic') | (df['prediction'] == 'authentic')\n",
        "    df['image_score'] = ((df['label'] == df['prediction']) & authentic_indices).astype(float)\n",
        "\n",
        "    def eval_wrapped(row):\n",
        "        result = evaluate_single_image(row['label'], row['prediction'], row['shape'])\n",
        "        print(f\"finished one image, image_score: {result}\")\n",
        "        return result\n",
        "\n",
        "    df.loc[~authentic_indices, 'image_score'] = df.loc[~authentic_indices].apply(\n",
        "        eval_wrapped, axis=1\n",
        "    )\n",
        "    return float(np.mean(df['image_score']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e2ce2a8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation forged-only: 100%|██████████| 1027/1027 [01:33<00:00, 10.96it/s]\n"
          ]
        }
      ],
      "source": [
        "solution = []\n",
        "submission = []\n",
        "\n",
        "val_items = [(p, \"forged\") for p in val_forg]\n",
        "val_items += [(p, \"authentic\") for p in val_auth]\n",
        "results = []\n",
        "\n",
        "for p, status in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "    pil = Image.open(p).convert(\"RGB\")\n",
        "    label, masks_pred, dbg = pipeline_final(pil)  # masks_pred is now a LIST of masks\n",
        "\n",
        "    m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "    \n",
        "    # Convert ground truth to list if it's 3D (N, H, W), otherwise wrap in list\n",
        "    if m_gt.ndim == 3:\n",
        "        gt_masks_list = [m_gt[i] for i in range(m_gt.shape[0])]\n",
        "        our_shape = m_gt.shape[1:]\n",
        "    else:\n",
        "        gt_masks_list = [m_gt]\n",
        "        our_shape = m_gt.shape\n",
        "    \n",
        "    # Prepare predicted masks list - resize to match ground truth shape if needed\n",
        "    if label == \"authentic\" or len(masks_pred) == 0:\n",
        "        pred_masks_list = []\n",
        "    else:\n",
        "        pred_masks_list = []\n",
        "        for m in masks_pred:\n",
        "            m_resized = cv2.resize((m > 0).astype(np.uint8), (our_shape[1], our_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "            if m_resized.sum() > 0:  # Only add non-empty masks\n",
        "                pred_masks_list.append(m_resized)\n",
        "    \n",
        "    # Encode using the list-based rle_encode (joins with ';')\n",
        "    solution_ann = \"authentic\" if status == \"authentic\" else rle_encode(gt_masks_list)\n",
        "    submission_ann = \"authentic\" if len(pred_masks_list) == 0 else rle_encode(pred_masks_list)\n",
        "\n",
        "    solution.append({ \"annotation\": str(solution_ann), \"shape\": str(list(our_shape)), \"image\": p})\n",
        "    submission.append({ \"annotation\": str(submission_ann), \"shape\": str(list(our_shape)), \"image\": p})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e42e8acf",
      "metadata": {},
      "outputs": [],
      "source": [
        "submissionDF = pd.DataFrame(submission)\n",
        "solutionDF = pd.DataFrame(solution)\n",
        "# make row_id column\n",
        "submissionDF['row_id'] = submissionDF.index\n",
        "solutionDF['row_id'] = solutionDF.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f4e786a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating 507 forged images on GPU...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU Scoring: 100%|██████████| 507/507 [00:07<00:00, 67.47it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Completed: 1027 succeeded, 0 failed\n",
            "📊 Final score: 0.2699\n",
            "\n",
            "Final Score: 0.2699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Use GPU-accelerated scoring (returns score and detailed results DataFrame)\n",
        "final_score, results_df = score_gpu(solutionDF, submissionDF, \"row_id\", device=device)\n",
        "\n",
        "print(f\"\\nFinal Score: {final_score:.4f}\")\n",
        "\n",
        "# Show failed rows for debugging\n",
        "failed_df = results_df[~results_df['success']]\n",
        "if len(failed_df) > 0:\n",
        "    print(f\"\\n❌ Failed rows ({len(failed_df)}):\")\n",
        "    print(failed_df[['row_id', 'eval_type', 'error_msg', 'num_gt_masks', 'num_pred_masks', 'shape_parsed']].to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edfc0878",
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6d7921",
      "metadata": {},
      "outputs": [],
      "source": [
        "index = 984\n",
        "image = Image.open(results_df.iloc[index]['image'])\n",
        "\n",
        "results_df.iloc[index]\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370ba7a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "our_shape = json.loads(results_df.iloc[index]['shape'])\n",
        "gt_mask = [rle_decode(x, our_shape) for x in results_df.iloc[index]['label'].split(';')][0]\n",
        "pred_mask = [rle_decode(x, our_shape) for x in results_df.iloc[index]['prediction'].split(';')][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d80d5f62",
      "metadata": {},
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e99496",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(image)\n",
        "plt.imshow(pred_mask, alpha=0.2)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2985451d",
      "metadata": {},
      "outputs": [],
      "source": [
        "submissionDF[submissionDF.row_id ==40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc1b343",
      "metadata": {},
      "outputs": [],
      "source": [
        "submissionDF.iloc[2]['annotation'].split(';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35d4d01",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-11T01:39:57.424783Z",
          "iopub.status.busy": "2025-11-11T01:39:57.424007Z",
          "iopub.status.idle": "2025-11-11T01:39:58.507865Z",
          "shell.execute_reply": "2025-11-11T01:39:58.507093Z"
        },
        "papermill": {
          "duration": 1.818846,
          "end_time": "2025-11-11T01:39:58.509513",
          "exception": false,
          "start_time": "2025-11-11T01:39:56.690667",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os, json, cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- RLE Encoder for Kaggle Submission (handles MULTIPLE masks) ---\n",
        "def rle_encode_single(mask: np.ndarray, fg_val: int = 1) -> str:\n",
        "    \"\"\"Encode a single 2D mask to RLE JSON string.\"\"\"\n",
        "    pixels = mask.T.flatten()\n",
        "    dots = np.where(pixels == fg_val)[0]\n",
        "    if len(dots) == 0:\n",
        "        return None  # Empty mask\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return json.dumps([int(x) for x in run_lengths])\n",
        "\n",
        "def rle_encode_multi(masks: list, fg_val: int = 1) -> str:\n",
        "    \"\"\"Encode multiple masks, joining with semicolons (competition format).\"\"\"\n",
        "    encoded = []\n",
        "    for m in masks:\n",
        "        enc = rle_encode_single((m > 0).astype(np.uint8), fg_val)\n",
        "        if enc is not None:\n",
        "            encoded.append(enc)\n",
        "    return ';'.join(encoded) if encoded else \"authentic\"\n",
        "\n",
        "# --- Paths ---\n",
        "TEST_DIR = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\n",
        "SAMPLE_SUB = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\n",
        "OUT_PATH = \"submission.csv\"\n",
        "\n",
        "rows = []\n",
        "for f in tqdm(sorted(os.listdir(TEST_DIR)), desc=\"Inference on Test Set\"):\n",
        "    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n",
        "    label, masks, dbg = pipeline_final(pil)  # masks is now a LIST of masks\n",
        "\n",
        "    # Annotation finale\n",
        "    if label == \"authentic\" or len(masks) == 0:\n",
        "        annot = \"authentic\"\n",
        "    else:\n",
        "        annot = rle_encode_multi(masks)\n",
        "\n",
        "    rows.append({\n",
        "        \"case_id\": Path(f).stem,\n",
        "        \"annotation\": annot,\n",
        "        \"area\": int(dbg.get(\"area\", 0)),\n",
        "        \"mean\": float(dbg.get(\"mean_inside\", 0.0)),\n",
        "        \"thr\": float(dbg.get(\"thr\", 0.0)),\n",
        "        \"num_masks\": int(dbg.get(\"num_masks\", len(masks) if masks else 0))\n",
        "    })\n",
        "\n",
        "\n",
        "sub = pd.DataFrame(rows)\n",
        "ss = pd.read_csv(SAMPLE_SUB)\n",
        "ss[\"case_id\"] = ss[\"case_id\"].astype(str)\n",
        "sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n",
        "final = ss[[\"case_id\"]].merge(sub, on=\"case_id\", how=\"left\")\n",
        "final[\"annotation\"] = final[\"annotation\"].fillna(\"authentic\")\n",
        "final[[\"case_id\", \"annotation\"]].to_csv(OUT_PATH, index=False)\n",
        "\n",
        "print(f\"\\n✅ Saved submission file: {OUT_PATH}\")\n",
        "print(final.head(10))\n",
        "\n",
        "\n",
        "sample_files = sorted(os.listdir(TEST_DIR))[:5]\n",
        "for f in sample_files:\n",
        "    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n",
        "    label, masks, dbg = pipeline_final(pil)\n",
        "    \n",
        "    # Combine all masks for visualization (overlay them)\n",
        "    if label == \"authentic\" or len(masks) == 0:\n",
        "        combined_mask = np.zeros(pil.size[::-1], np.uint8)\n",
        "    else:\n",
        "        combined_mask = np.zeros(pil.size[::-1], np.uint8)\n",
        "        for m in masks:\n",
        "            combined_mask = np.maximum(combined_mask, (m > 0).astype(np.uint8))\n",
        "\n",
        "    print(f\"{'🔴' if label=='forged' else '🟢'} {f}: {label} | area={dbg.get('area', 0)} mean={dbg.get('mean_inside', 0):.3f} | num_masks={len(masks) if masks else 0}\")\n",
        "\n",
        "    if label == \"authentic\":\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(pil)\n",
        "        plt.title(f\"{f} — Authentic\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.imshow(pil)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.imshow(pil)\n",
        "        plt.imshow(combined_mask, alpha=0.45, cmap=\"Reds\")\n",
        "        plt.title(f\"Predicted Forged Masks ({len(masks)})\\nArea={dbg.get('area', 0)} | Mean={dbg.get('mean_inside', 0):.3f}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2fd92842",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 Created local test directory structure\n",
            "\n",
            "📷 Copying validation images to test directory...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying images: 100%|██████████| 1027/1027 [00:43<00:00, 23.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Copied 1027 images to ./data/test_images\n",
            "\n",
            "🎯 Saving ground truth for scoring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving ground truth: 100%|██████████| 1027/1027 [00:34<00:00, 29.99it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Saved ground truth for 1027 images\n",
            "   Forged: 551, Authentic: 476\n",
            "\n",
            "✅ Local test environment ready!\n",
            "   Test images: ./data/test_images\n",
            "   Ground truth: ./data/ground_truth\n",
            "\n",
            "   Now run submission.ipynb with LOCAL_MODE = True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==================== PREPARE LOCAL TEST ENVIRONMENT ====================\n",
        "# This cell copies validation images to test_images so you can run submission.ipynb locally\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Local paths (matches submission.ipynb LOCAL_MODE)\n",
        "LOCAL_TEST_DIR = \"./data/test_images\"\n",
        "LOCAL_GT_DIR = \"./data/ground_truth\"\n",
        "\n",
        "# Clear and create directories\n",
        "if os.path.exists(LOCAL_TEST_DIR):\n",
        "    shutil.rmtree(LOCAL_TEST_DIR)\n",
        "os.makedirs(LOCAL_TEST_DIR, exist_ok=True)\n",
        "os.makedirs(LOCAL_GT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"📁 Created local test directory structure\")\n",
        "\n",
        "# 1. Copy validation images to test directory\n",
        "print(\"\\n📷 Copying validation images to test directory...\")\n",
        "val_all = val_forg + val_auth\n",
        "for p in tqdm(val_all, desc=\"Copying images\"):\n",
        "    dst = os.path.join(LOCAL_TEST_DIR, os.path.basename(p))\n",
        "    shutil.copy2(p, dst)\n",
        "print(f\"   Copied {len(val_all)} images to {LOCAL_TEST_DIR}\")\n",
        "\n",
        "# 2. Save ground truth for scoring\n",
        "print(\"\\n🎯 Saving ground truth for scoring...\")\n",
        "gt_data = []\n",
        "for p in tqdm(val_all, desc=\"Saving ground truth\"):\n",
        "    case_id = Path(p).stem\n",
        "    mask_path = os.path.join(MASK_DIR, f\"{case_id}.npy\")\n",
        "    \n",
        "    if p in val_forg and os.path.exists(mask_path):\n",
        "        # Forged image - load and save mask\n",
        "        masks = np.load(mask_path)\n",
        "        np.save(os.path.join(LOCAL_GT_DIR, f\"{case_id}.npy\"), masks)\n",
        "        gt_data.append({\n",
        "            \"case_id\": case_id,\n",
        "            \"label\": \"forged\",\n",
        "            \"mask_file\": f\"{case_id}.npy\",\n",
        "            \"shape\": list(masks.shape[1:]) if masks.ndim == 3 else list(masks.shape)\n",
        "        })\n",
        "    else:\n",
        "        # Authentic image\n",
        "        img = Image.open(p)\n",
        "        gt_data.append({\n",
        "            \"case_id\": case_id,\n",
        "            \"label\": \"authentic\",\n",
        "            \"mask_file\": None,\n",
        "            \"shape\": [img.height, img.width]\n",
        "        })\n",
        "\n",
        "gt_df = pd.DataFrame(gt_data)\n",
        "gt_df.to_csv(os.path.join(LOCAL_GT_DIR, \"ground_truth.csv\"), index=False)\n",
        "print(f\"   Saved ground truth for {len(gt_df)} images\")\n",
        "print(f\"   Forged: {(gt_df['label'] == 'forged').sum()}, Authentic: {(gt_df['label'] == 'authentic').sum()}\")\n",
        "\n",
        "print(\"\\n✅ Local test environment ready!\")\n",
        "print(f\"   Test images: {LOCAL_TEST_DIR}\")\n",
        "print(f\"   Ground truth: {LOCAL_GT_DIR}\")\n",
        "print(\"\\n   Now run submission.ipynb with LOCAL_MODE = True\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 14174843,
          "isSourceIdPinned": false,
          "sourceId": 113558,
          "sourceType": "competition"
        },
        {
          "isSourceIdPinned": false,
          "modelId": 986,
          "modelInstanceId": 3326,
          "sourceId": 4534,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 31154,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6449.810614,
      "end_time": "2025-11-11T01:40:11.393133",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-11-10T23:52:41.582519",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
