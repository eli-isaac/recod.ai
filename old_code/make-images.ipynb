{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c3c3bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from transformers import Sam3Processor, Sam3Model\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "import cv2\n",
        "from itertools import combinations\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from scipy.ndimage import label\n",
        "from typing import Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce6db0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports for embedding models (loaded on-demand to save GPU memory)\n",
        "from transformers import CLIPProcessor, CLIPModel, AutoImageProcessor, AutoModel, AutoProcessor\n",
        "import open_clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787de1fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n",
        "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e98836",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Sam3Results:\n",
        "    masks: np.ndarray\n",
        "    boxes: np.ndarray\n",
        "    scores: np.ndarray\n",
        "\n",
        "@dataclass\n",
        "class Sample:\n",
        "    image: Image.Image\n",
        "    samResults: Sam3Results\n",
        "\n",
        "@dataclass\n",
        "class MyImage:\n",
        "    image: Image.Image\n",
        "    file_name: str\n",
        "    index: int\n",
        "    size: int\n",
        "    samResults: Union[Sam3Results, None] = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7fe4b58",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sam3_results(\n",
        "    model: Sam3Model,\n",
        "    processor: Sam3Processor,\n",
        "    # List of PIL images\n",
        "    my_images: list[MyImage],\n",
        "    batch_size: int = 16,\n",
        ") -> list[int]:\n",
        "\n",
        "    new_samples = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(my_images):\n",
        "        batch: list[MyImage] = []\n",
        "\n",
        "        while i < len(my_images) and len(batch) < batch_size:\n",
        "            batch.append(my_images[i])\n",
        "            i += 1\n",
        "\n",
        "        images = [my_image.image for my_image in batch]\n",
        "\n",
        "        inputs = processor(images=images, text=[\"distinct object\"] * len(images), return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            results_batch = processor.post_process_instance_segmentation(\n",
        "                outputs,\n",
        "                threshold=0.5,\n",
        "                mask_threshold=0.4,\n",
        "                target_sizes=inputs.get(\"original_sizes\").tolist()\n",
        "            )\n",
        "\n",
        "        for my_image, results in zip(batch, results_batch):\n",
        "            my_image.samResults = Sam3Results(\n",
        "                    masks=results[\"masks\"].detach().cpu().numpy(),\n",
        "                    boxes=results[\"boxes\"].detach().cpu().numpy(),\n",
        "                    scores=results[\"scores\"].detach().cpu().numpy()\n",
        "                )\n",
        "\n",
        "        # Clean up GPU memory after each batch\n",
        "        del inputs, outputs, results_batch\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"Processed {i}/{len(my_images)} samples\")\n",
        "\n",
        "    return my_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c862105a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# get a list of images from a folder\n",
        "def get_images_from_folder(folder_path: str) -> list[MyImage]:\n",
        "    return [MyImage(Image.open(os.path.join(folder_path, f)), f, i, os.path.getsize(os.path.join(folder_path, f))) for i, f in enumerate(os.listdir(folder_path))]\n",
        "\n",
        "\n",
        "\n",
        "authentic_images = get_images_from_folder(\"./train_images/authentic/\")\n",
        "len(authentic_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fe49a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "authentic_images = [image for image in authentic_images if image.size < 2*10**6]\n",
        "len(authentic_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85292e28",
      "metadata": {},
      "outputs": [],
      "source": [
        "sam3_results = get_sam3_results(model, processor, authentic_images[:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d7d3b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "example = 8\n",
        "sample_image = sam3_results[example]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353f42f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_candidate_masks(sample_image: MyImage):\n",
        "    candidate_masks = []\n",
        "    image_size = sample_image.size\n",
        "    for mask in sample_image.samResults.masks:\n",
        "        mask_size = np.sum(mask)\n",
        "        mask_area_ratio = mask_size / image_size\n",
        "        if mask_area_ratio > 0.005 and mask_area_ratio < 0.2:\n",
        "            candidate_masks.append(mask)\n",
        "    return candidate_masks\n",
        "candidate_masks = get_candidate_masks(sample_image)\n",
        "candidate_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf917dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy-paste multiple objects with overlap prevention toggle (FIXED)\n",
        "def copy_paste_objects(image, masks, num_copies_list, prevent_overlap=True):\n",
        "    \"\"\"masks: list of masks, num_copies_list: list of copy counts per object\n",
        "    prevent_overlap: if True, objects won't overlap with each other\n",
        "    Returns: (forged_image, list_of_masks) - one mask per object\"\"\"\n",
        "    img = np.array(image)\n",
        "    h, w = img.shape[:2]\n",
        "    occupied = np.zeros((h, w), dtype=bool)\n",
        "    result = img.copy()\n",
        "    object_masks = []\n",
        "\n",
        "    # Mark all original masks as occupied first\n",
        "    if prevent_overlap:\n",
        "        for mask in masks:\n",
        "            occupied[mask.astype(bool)] = True\n",
        "\n",
        "    for mask, num_copies in zip(masks, num_copies_list):\n",
        "        mask_bool = mask.astype(bool)\n",
        "        rows = np.any(mask_bool, axis=1)\n",
        "        cols = np.any(mask_bool, axis=0)\n",
        "        y_min, y_max = np.where(rows)[0][[0, -1]]\n",
        "        x_min, x_max = np.where(cols)[0][[0, -1]]\n",
        "        obj_h, obj_w = y_max - y_min + 1, x_max - x_min + 1\n",
        "\n",
        "        object_crop = img[y_min:y_max+1, x_min:x_max+1]\n",
        "        mask_crop = mask_bool[y_min:y_max+1, x_min:x_max+1]\n",
        "        obj_mask = mask_bool.copy()\n",
        "\n",
        "        for _ in range(num_copies):\n",
        "            for _ in range(1000):\n",
        "                offset_x = random.randint(0, max(1, w - obj_w))\n",
        "                offset_y = random.randint(0, max(1, h - obj_h))\n",
        "\n",
        "                paste_region = occupied[offset_y:offset_y+obj_h, offset_x:offset_x+obj_w]\n",
        "                if not prevent_overlap or not np.any(paste_region[mask_crop]):\n",
        "                    result[offset_y:offset_y+obj_h, offset_x:offset_x+obj_w][mask_crop] = object_crop[mask_crop]\n",
        "                    obj_mask[offset_y:offset_y+obj_h, offset_x:offset_x+obj_w] = mask_crop\n",
        "                    if prevent_overlap:\n",
        "                        occupied[offset_y:offset_y+obj_h, offset_x:offset_x+obj_w][mask_crop] = True\n",
        "                    break\n",
        "\n",
        "        object_masks.append(obj_mask)\n",
        "\n",
        "    return Image.fromarray(result), object_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c8ac1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_image, new_mask = copy_paste_objects(sample_image.image, candidate_masks[:2], [2,1])\n",
        "new_image.save(\"./ourtraining/duplicate_2.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bd79e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "variantions = [\n",
        "    {\n",
        "        \"name\": \"1.1\",\n",
        "        \"num_copies\": [1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1.2\",\n",
        "        \"num_copies\": [1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1.3\",\n",
        "        \"num_copies\": [1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"2.1\",\n",
        "        \"num_copies\": [2]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"2.2\",\n",
        "        \"num_copies\": [2]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"3\",\n",
        "        \"num_copies\": [3]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1.1\",\n",
        "        \"num_copies\": [1,1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1.2\",\n",
        "        \"num_copies\": [1,1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-2\",\n",
        "        \"num_copies\": [1,2]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"2-1\",\n",
        "        \"num_copies\": [2,1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"2-2\",\n",
        "        \"num_copies\": [2,2]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-3\",\n",
        "        \"num_copies\": [1,3]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"2-3\",\n",
        "        \"num_copies\": [2,3]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1-1\",\n",
        "        \"num_copies\": [1,1,1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1-2\",\n",
        "        \"num_copies\": [1,1,2]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1-3\",\n",
        "        \"num_copies\": [1,1,3]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-3-3\",\n",
        "        \"num_copies\": [1,3,3]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1-1-1\",\n",
        "        \"num_copies\": [1,1,1,1]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1-1-2\",\n",
        "        \"num_copies\": [1,1,1,2]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"1-1-1-3\",\n",
        "        \"num_copies\": [1,1,1,3]\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce156ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, image in enumerate(sam3_results[993:]):\n",
        "    try:\n",
        "        print(f\"Processing {i+1}/{len(sam3_results)}\")\n",
        "        candidate_masks = get_candidate_masks(image)\n",
        "        if len(candidate_masks) > 0:\n",
        "            for variation in variantions:\n",
        "                file_name = image.file_name.split(\".\")[0]\n",
        "                # get random masks from candidate_masks based on variation[\"num_copies\"]\n",
        "                # it should be random with replacement\n",
        "                k = len(variation[\"num_copies\"]) if len(variation[\"num_copies\"]) <= len(candidate_masks) else len(candidate_masks)\n",
        "                random_masks = random.sample(candidate_masks, k=k)\n",
        "                new_image, new_mask = copy_paste_objects(image.image, random_masks, variation[\"num_copies\"])\n",
        "                new_image.save(f\"./ourtraining/images/{file_name}_{variation['name']}.png\")\n",
        "                np.save(f\"./ourtraining/masks/{file_name}_{variation['name']}.npy\", new_mask)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {i+1}/{len(sam3_results)}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64fa48f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "name  = '2430_2-2'\n",
        "image = Image.open(f\"./ourtraining/images/{name}.png\")\n",
        "authentic_image = Image.open(f\"./train_images/authentic/{name.split('_')[0]}.png\")\n",
        "plt.imshow(authentic_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "masks = np.load(f\"./ourtraining/masks/{name}.npy\")\n",
        "\n",
        "for mask in masks:\n",
        "    # show the image and the mask side by side\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask, alpha=0.2)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07023cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login, upload_folder\n",
        "\n",
        "# (optional) Login with your Hugging Face credentials\n",
        "login()\n",
        "\n",
        "# Push your dataset files\n",
        "upload_folder(folder_path=\"./ourtraining\", repo_id=\"eliplutchok/recod_comp\", repo_type=\"dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a54a0501",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EASIER WAY 1: Using Hugging Face CLI (SIMPLEST - recommended!)\n",
        "# For large folders, use the large folder upload command:\n",
        "# huggingface-cli upload-large-folder eliplutchok/recod_comp ./ourtraining --repo-type dataset\n",
        "\n",
        "# EASIER WAY 2: Using upload_large_folder in Python (for large datasets)\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "# Login (only needed once per session)\n",
        "# Option A: Use token from environment variable (recommended)\n",
        "# os.environ[\"HF_TOKEN\"] = \"your_token_here\"  # Get from https://huggingface.co/settings/tokens\n",
        "\n",
        "# Option B: Use login() - may need ipywidgets: pip install ipywidgets\n",
        "# from huggingface_hub import login\n",
        "# login()\n",
        "\n",
        "# Create API instance\n",
        "api = HfApi()\n",
        "\n",
        "# Upload large folder - handles large datasets better with progress tracking\n",
        "api.upload_large_folder(\n",
        "    folder_path=\"./ourtraining\",\n",
        "    repo_id=\"eliplutchok/recod_comp\",\n",
        "    repo_type=\"dataset\",\n",
        "    ignore_patterns=[\"*.ipynb_checkpoints\", \"__pycache__\", \"*.pyc\"]  # Skip unnecessary files\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d87ab0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
