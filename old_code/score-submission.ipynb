{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Score Submission\n",
        "\n",
        "This notebook scores the submission.csv generated by submission.ipynb against the validation ground truth.\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Run the \"Prepare Local Test Environment\" cell in cnn-dinov2-hybrid.ipynb\n",
        "2. Run submission.ipynb (with LOCAL_MODE = True) to generate submission.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numba\n",
        "import scipy.optimize\n",
        "import numpy.typing as npt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Found submission.csv\n",
            "âœ… Found ground truth at ./data/ground_truth\n"
          ]
        }
      ],
      "source": [
        "# ==================== PATHS ====================\n",
        "SUBMISSION_PATH = \"submission.csv\"\n",
        "GT_DIR = \"./data/ground_truth\"  # Local path (matches cnn-dinov2-hybrid.ipynb setup)\n",
        "GT_CSV = os.path.join(GT_DIR, \"ground_truth.csv\")\n",
        "\n",
        "# Check files exist\n",
        "assert os.path.exists(SUBMISSION_PATH), f\"submission.csv not found. Run submission.ipynb first!\"\n",
        "assert os.path.exists(GT_CSV), f\"Ground truth not found at {GT_CSV}. Run the setup cell in cnn-dinov2-hybrid.ipynb first!\"\n",
        "\n",
        "print(f\"âœ… Found submission.csv\")\n",
        "print(f\"âœ… Found ground truth at {GT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== RLE DECODE ====================\n",
        "\n",
        "@numba.njit\n",
        "def _rle_decode_jit(mask_rle: npt.NDArray, height: int, width: int) -> npt.NDArray:\n",
        "    if len(mask_rle) % 2 != 0:\n",
        "        raise ValueError('Odd number of values')\n",
        "    starts, lengths = mask_rle[0::2], mask_rle[1::2]\n",
        "    starts = starts - 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(height * width, dtype=np.bool_)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img\n",
        "\n",
        "\n",
        "def rle_decode(mask_rle: str, shape: tuple) -> npt.NDArray:\n",
        "    mask_rle = json.loads(mask_rle)\n",
        "    mask_rle = np.asarray(mask_rle, dtype=np.int32)\n",
        "    return _rle_decode_jit(mask_rle, shape[0], shape[1]).reshape(shape, order='F')\n",
        "\n",
        "\n",
        "@numba.jit(nopython=True)\n",
        "def _rle_encode_jit(x: npt.NDArray, fg_val: int = 1):\n",
        "    dots = np.where(x.T.flatten() == fg_val)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "\n",
        "def rle_encode(masks: list, fg_val: int = 1) -> str:\n",
        "    return ';'.join([json.dumps(_rle_encode_jit(x, fg_val)) for x in masks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== SCORING FUNCTIONS ====================\n",
        "\n",
        "def calculate_f1_score(pred_mask: npt.NDArray, gt_mask: npt.NDArray):\n",
        "    pred_flat = pred_mask.flatten()\n",
        "    gt_flat = gt_mask.flatten()\n",
        "    tp = np.sum((pred_flat == 1) & (gt_flat == 1))\n",
        "    fp = np.sum((pred_flat == 1) & (gt_flat == 0))\n",
        "    fn = np.sum((pred_flat == 0) & (gt_flat == 1))\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    if (precision + recall) > 0:\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def calculate_f1_matrix(pred_masks: list, gt_masks: list):\n",
        "    num_pred = len(pred_masks)\n",
        "    num_gt = len(gt_masks)\n",
        "    f1_matrix = np.zeros((max(num_pred, num_gt), num_gt))\n",
        "    for i in range(num_pred):\n",
        "        for j in range(num_gt):\n",
        "            f1_matrix[i, j] = calculate_f1_score(pred_masks[i], gt_masks[j])\n",
        "    return f1_matrix\n",
        "\n",
        "\n",
        "def oF1_score(pred_masks: list, gt_masks: list):\n",
        "    if len(pred_masks) == 0 or len(gt_masks) == 0:\n",
        "        return 0.0\n",
        "    f1_matrix = calculate_f1_matrix(pred_masks, gt_masks)\n",
        "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-f1_matrix)\n",
        "    excess_penalty = len(gt_masks) / max(len(pred_masks), len(gt_masks))\n",
        "    return np.mean(f1_matrix[row_ind, col_ind]) * excess_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded submission: 929 rows\n",
            "Loaded ground truth: 1027 rows\n",
            "Merged: 1027 rows\n",
            "\n",
            "Ground truth distribution:\n",
            "  Forged: 551\n",
            "  Authentic: 476\n"
          ]
        }
      ],
      "source": [
        "# ==================== LOAD DATA ====================\n",
        "\n",
        "# Load submission\n",
        "submission_df = pd.read_csv(SUBMISSION_PATH)\n",
        "submission_df['case_id'] = submission_df['case_id'].astype(str)\n",
        "print(f\"Loaded submission: {len(submission_df)} rows\")\n",
        "\n",
        "# Load ground truth\n",
        "gt_df = pd.read_csv(GT_CSV)\n",
        "gt_df['case_id'] = gt_df['case_id'].astype(str)\n",
        "print(f\"Loaded ground truth: {len(gt_df)} rows\")\n",
        "\n",
        "# Merge\n",
        "merged = gt_df.merge(submission_df, on='case_id', how='left')\n",
        "merged['annotation'] = merged['annotation'].fillna('authentic')\n",
        "print(f\"Merged: {len(merged)} rows\")\n",
        "\n",
        "# Show label distribution\n",
        "print(f\"\\nGround truth distribution:\")\n",
        "print(f\"  Forged: {(gt_df['label'] == 'forged').sum()}\")\n",
        "print(f\"  Authentic: {(gt_df['label'] == 'authentic').sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scoring: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1027/1027 [00:16<00:00, 64.02it/s]\n"
          ]
        }
      ],
      "source": [
        "# ==================== SCORE ====================\n",
        "\n",
        "scores = []\n",
        "details = []\n",
        "\n",
        "for idx, row in tqdm(merged.iterrows(), total=len(merged), desc=\"Scoring\"):\n",
        "    case_id = row['case_id']\n",
        "    gt_label = row['label']\n",
        "    pred_annotation = row['annotation']\n",
        "    shape = json.loads(row['shape']) if isinstance(row['shape'], str) else row['shape']\n",
        "    \n",
        "    # Case 1: Both authentic\n",
        "    if gt_label == 'authentic' and pred_annotation == 'authentic':\n",
        "        score = 1.0\n",
        "        detail = \"TN (both authentic)\"\n",
        "    \n",
        "    # Case 2: GT authentic, Pred forged (false positive)\n",
        "    elif gt_label == 'authentic' and pred_annotation != 'authentic':\n",
        "        score = 0.0\n",
        "        detail = \"FP (predicted forged, actually authentic)\"\n",
        "    \n",
        "    # Case 3: GT forged, Pred authentic (false negative)\n",
        "    elif gt_label == 'forged' and pred_annotation == 'authentic':\n",
        "        score = 0.0\n",
        "        detail = \"FN (predicted authentic, actually forged)\"\n",
        "    \n",
        "    # Case 4: Both forged - compute oF1\n",
        "    else:\n",
        "        # Load ground truth masks\n",
        "        mask_file = row['mask_file']\n",
        "        gt_masks_arr = np.load(os.path.join(GT_DIR, mask_file))\n",
        "        if gt_masks_arr.ndim == 2:\n",
        "            gt_masks = [gt_masks_arr]\n",
        "        else:\n",
        "            gt_masks = [gt_masks_arr[i] for i in range(gt_masks_arr.shape[0])]\n",
        "        \n",
        "        # Decode predicted masks\n",
        "        pred_rles = pred_annotation.split(';')\n",
        "        pred_masks = [rle_decode(rle, tuple(shape)) for rle in pred_rles]\n",
        "        \n",
        "        score = oF1_score(pred_masks, gt_masks)\n",
        "        detail = f\"oF1 (pred={len(pred_masks)} masks, gt={len(gt_masks)} masks)\"\n",
        "    \n",
        "    scores.append(score)\n",
        "    details.append(detail)\n",
        "\n",
        "merged['score'] = scores\n",
        "merged['detail'] = details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“Š FINAL SCORE: 0.2610\n",
            "============================================================\n",
            "\n",
            "ðŸ“ˆ Score Breakdown:\n",
            "   Total images: 1027\n",
            "   Mean score: 0.2610\n",
            "   Std score: 0.3720\n",
            "   Min score: 0.0000\n",
            "   Max score: 1.0000\n",
            "\n",
            "ðŸ“‹ By Category:\n",
            "   oF1 (pred=1 masks, gt=1 masks): 189 images, avg score = 0.2385\n",
            "   FN (predicted authentic, actually forged): 76 images, avg score = 0.0000\n",
            "   oF1 (pred=2 masks, gt=2 masks): 66 images, avg score = 0.1766\n",
            "   oF1 (pred=1 masks, gt=3 masks): 12 images, avg score = 0.0125\n",
            "   oF1 (pred=2 masks, gt=1 masks): 153 images, avg score = 0.1714\n",
            "   oF1 (pred=1 masks, gt=2 masks): 50 images, avg score = 0.0315\n",
            "   oF1 (pred=2 masks, gt=3 masks): 4 images, avg score = 0.0856\n",
            "   oF1 (pred=2 masks, gt=4 masks): 1 images, avg score = 0.0189\n",
            "   FP (predicted forged, actually authentic): 293 images, avg score = 0.0000\n",
            "   TN (both authentic): 183 images, avg score = 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ==================== RESULTS ====================\n",
        "\n",
        "final_score = np.mean(scores)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"ðŸ“Š FINAL SCORE: {final_score:.4f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Breakdown\n",
        "print(f\"\\nðŸ“ˆ Score Breakdown:\")\n",
        "print(f\"   Total images: {len(merged)}\")\n",
        "print(f\"   Mean score: {final_score:.4f}\")\n",
        "print(f\"   Std score: {np.std(scores):.4f}\")\n",
        "print(f\"   Min score: {np.min(scores):.4f}\")\n",
        "print(f\"   Max score: {np.max(scores):.4f}\")\n",
        "\n",
        "# By category\n",
        "print(f\"\\nðŸ“‹ By Category:\")\n",
        "for detail_type in merged['detail'].unique():\n",
        "    subset = merged[merged['detail'] == detail_type]\n",
        "    print(f\"   {detail_type}: {len(subset)} images, avg score = {subset['score'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âŒ Worst 10 Predictions:\n",
            "case_id  label  score                                    detail\n",
            "  30587 forged    0.0 FN (predicted authentic, actually forged)\n",
            "   2087 forged    0.0 FN (predicted authentic, actually forged)\n",
            "  41125 forged    0.0 FN (predicted authentic, actually forged)\n",
            "   7703 forged    0.0 FN (predicted authentic, actually forged)\n",
            "  14692 forged    0.0 FN (predicted authentic, actually forged)\n",
            "  56106 forged    0.0 FN (predicted authentic, actually forged)\n",
            "   5902 forged    0.0            oF1 (pred=1 masks, gt=1 masks)\n",
            "  21004 forged    0.0 FN (predicted authentic, actually forged)\n",
            "  33778 forged    0.0 FN (predicted authentic, actually forged)\n",
            "   8214 forged    0.0 FN (predicted authentic, actually forged)\n"
          ]
        }
      ],
      "source": [
        "# ==================== WORST PREDICTIONS ====================\n",
        "\n",
        "print(\"\\nâŒ Worst 10 Predictions:\")\n",
        "worst = merged.nsmallest(10, 'score')[['case_id', 'label', 'score', 'detail']]\n",
        "print(worst.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”¢ Confusion Matrix (Classification):\n",
            "pred_label  authentic  forged   All\n",
            "label                              \n",
            "authentic         183     293   476\n",
            "forged             76     475   551\n",
            "All               259     768  1027\n"
          ]
        }
      ],
      "source": [
        "# ==================== CONFUSION MATRIX ====================\n",
        "\n",
        "# Simplified: authentic vs forged prediction\n",
        "merged['pred_label'] = merged['annotation'].apply(lambda x: 'authentic' if x == 'authentic' else 'forged')\n",
        "\n",
        "print(\"\\nðŸ”¢ Confusion Matrix (Classification):\")\n",
        "print(pd.crosstab(merged['label'], merged['pred_label'], margins=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ’¾ Saved detailed results to scoring_results.csv\n"
          ]
        }
      ],
      "source": [
        "# ==================== SAVE DETAILED RESULTS ====================\n",
        "\n",
        "# Save for further analysis\n",
        "results_path = \"scoring_results.csv\"\n",
        "merged[['case_id', 'label', 'score', 'detail']].to_csv(results_path, index=False)\n",
        "print(f\"\\nðŸ’¾ Saved detailed results to {results_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
