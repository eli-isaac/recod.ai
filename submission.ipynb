{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Submission Notebook - DINOv2 Forgery Detection\n",
        "\n",
        "This notebook loads pre-trained weights and generates predictions for the test set.\n",
        "\n",
        "**Usage:**\n",
        "- Set `LOCAL_MODE = True` for local testing with validation data\n",
        "- Set `LOCAL_MODE = False` for Kaggle submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/cali-recod/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os, cv2, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoImageProcessor, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mode: LOCAL\n",
            "Device: cuda\n",
            "Test dir: ./data/test_images\n",
            "Weights: ./model_seg_final.pt\n"
          ]
        }
      ],
      "source": [
        "# ==================== CONFIG ====================\n",
        "\n",
        "# ⚡ SWITCH THIS FLAG FOR LOCAL VS KAGGLE MODE ⚡\n",
        "LOCAL_MODE = True  # Set to False when running on Kaggle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if LOCAL_MODE:\n",
        "    # Local paths (for testing with validation data)\n",
        "    BASE_DIR = \"./data\"\n",
        "    TEST_DIR = f\"{BASE_DIR}/test_images\"  # Or use validation images\n",
        "    SAMPLE_SUB = None  # Will create from test files\n",
        "    WEIGHTS_PATH = \"./model_seg_final.pt\"\n",
        "else:\n",
        "    # Kaggle paths\n",
        "    TEST_DIR = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\n",
        "    SAMPLE_SUB = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\n",
        "    WEIGHTS_PATH = \"/kaggle/input/msv-v1/model_seg_final.pt\"\n",
        "\n",
        "# Model config (must match training)\n",
        "DINO_PATH = \"facebook/dinov2-base\"\n",
        "IMG_SIZE = 512\n",
        "CHANNELS = 4\n",
        "\n",
        "OUT_PATH = \"submission.csv\"\n",
        "\n",
        "print(f\"Mode: {'LOCAL' if LOCAL_MODE else 'KAGGLE'}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Test dir: {TEST_DIR}\")\n",
        "print(f\"Weights: {WEIGHTS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== MODEL DEFINITION ====================\n",
        "\n",
        "class DinoDecoder(nn.Module):\n",
        "    \"\"\"Progressive upsampling decoder with regularization\"\"\"\n",
        "    def __init__(self, in_ch=768, out_ch=CHANNELS, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.up1 = self._block(in_ch, 384, dropout)\n",
        "        self.up2 = self._block(384, 192, dropout)\n",
        "        self.up3 = self._block(192, 96, dropout)\n",
        "        self.up4 = self._block(96, 48, dropout)\n",
        "        \n",
        "        self.final = nn.Conv2d(48, out_ch, kernel_size=1)\n",
        "    \n",
        "    def _block(self, in_ch, out_ch, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(dropout),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    \n",
        "    def forward(self, f, size):\n",
        "        x = F.interpolate(f, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up1(x)\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up2(x)\n",
        "        \n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = self.up3(x)\n",
        "        \n",
        "        x = F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
        "        x = self.up4(x)\n",
        "        \n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor, unfreeze_blocks=3):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        \n",
        "        # Freeze all parameters\n",
        "        for p in self.encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        # Unfreeze last N blocks\n",
        "        num_blocks = len(self.encoder.encoder.layer)\n",
        "        for i in range(num_blocks - unfreeze_blocks, num_blocks):\n",
        "            for p in self.encoder.encoder.layer[i].parameters():\n",
        "                p.requires_grad = True\n",
        "        \n",
        "        for p in self.encoder.layernorm.parameters():\n",
        "            p.requires_grad = True\n",
        "        \n",
        "        self.seg_head = DinoDecoder(768, CHANNELS)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        with torch.no_grad():\n",
        "            feats = self.encoder(**inputs).last_hidden_state\n",
        "        B, N, C = feats.shape\n",
        "        fmap = feats[:, 1:, :].permute(0, 2, 1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B, C, s, s)\n",
        "        return fmap\n",
        "\n",
        "    def forward_seg(self, x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap, (IMG_SIZE, IMG_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading DINOv2 encoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Loading weights: 100%|██████████| 223/223 [00:00<00:00, 674.05it/s, Materializing param=layernorm.weight]                                 \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "Loading weights from ./model_seg_final.pt...\n",
            "✅ Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==================== LOAD MODEL ====================\n",
        "\n",
        "print(\"Loading DINOv2 encoder...\")\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH).eval().to(device)\n",
        "\n",
        "print(\"Building model...\")\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "print(f\"Loading weights from {WEIGHTS_PATH}...\")\n",
        "model_seg.load_state_dict(torch.load(WEIGHTS_PATH, map_location=device))\n",
        "model_seg.eval()\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== INFERENCE FUNCTIONS ====================\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_all_channels(pil):\n",
        "    \"\"\"Returns probability maps for ALL channels.\"\"\"\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    return torch.sigmoid(model_seg.forward_seg(x))[0].cpu().numpy()\n",
        "\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.35):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    \"\"\"Returns a LIST of masks (one per detected forged region).\"\"\"\n",
        "    probs = segment_prob_map_all_channels(pil)\n",
        "    \n",
        "    all_masks = []\n",
        "    all_areas = []\n",
        "    all_means = []\n",
        "    all_thrs = []\n",
        "    \n",
        "    for ch in range(probs.shape[0]):\n",
        "        prob = probs[ch]\n",
        "        mask, thr = finalize_mask(prob, pil.size)\n",
        "        area = int(mask.sum())\n",
        "        \n",
        "        if area > 0:\n",
        "            prob_resized = cv2.resize(prob, pil.size, interpolation=cv2.INTER_LINEAR)\n",
        "            mean_inside = float(prob_resized[mask == 1].mean())\n",
        "        else:\n",
        "            mean_inside = 0.0\n",
        "        \n",
        "        # Filter out small/weak detections\n",
        "        if area >= 400 and mean_inside >= 0.35:\n",
        "            all_masks.append(mask)\n",
        "            all_areas.append(area)\n",
        "            all_means.append(mean_inside)\n",
        "            all_thrs.append(thr)\n",
        "    \n",
        "    if len(all_masks) == 0:\n",
        "        return \"authentic\", [], {\"area\": 0, \"mean_inside\": 0.0, \"thr\": 0.0}\n",
        "    \n",
        "    total_area = sum(all_areas)\n",
        "    avg_mean = sum(all_means) / len(all_means)\n",
        "    avg_thr = sum(all_thrs) / len(all_thrs)\n",
        "    \n",
        "    return \"forged\", all_masks, {\"area\": total_area, \"mean_inside\": avg_mean, \"thr\": avg_thr, \"num_masks\": len(all_masks)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== RLE ENCODING ====================\n",
        "\n",
        "def rle_encode_single(mask: np.ndarray, fg_val: int = 1) -> str:\n",
        "    \"\"\"Encode a single 2D mask to RLE JSON string.\"\"\"\n",
        "    pixels = mask.T.flatten()\n",
        "    dots = np.where(pixels == fg_val)[0]\n",
        "    if len(dots) == 0:\n",
        "        return None\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return json.dumps([int(x) for x in run_lengths])\n",
        "\n",
        "\n",
        "def rle_encode_multi(masks: list, fg_val: int = 1) -> str:\n",
        "    \"\"\"Encode multiple masks, joining with semicolons.\"\"\"\n",
        "    encoded = []\n",
        "    for m in masks:\n",
        "        enc = rle_encode_single((m > 0).astype(np.uint8), fg_val)\n",
        "        if enc is not None:\n",
        "            encoded.append(enc)\n",
        "    return ';'.join(encoded) if encoded else \"authentic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 929 test images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference: 100%|██████████| 929/929 [02:26<00:00,  6.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Saved submission to: submission.csv\n",
            "Total rows: 929\n",
            "Forged: 706\n",
            "Authentic: 223\n",
            "\n",
            "First 10 rows:\n",
            "  case_id                                         annotation\n",
            "0   10398                                          authentic\n",
            "1   10476                                          authentic\n",
            "2   10602  [35033, 11, 35516, 24, 36002, 33, 36492, 33, 3...\n",
            "3   10632  [3044, 7, 3298, 15, 3552, 18, 3807, 19, 4061, ...\n",
            "4   10636  [108, 58, 205, 40, 468, 58, 565, 40, 828, 58, ...\n",
            "5   10645  [29, 26, 149, 26, 269, 26, 389, 26, 509, 27, 6...\n",
            "6   10911  [103, 80, 359, 80, 612, 83, 869, 82, 1125, 82,...\n",
            "7   10939  [155, 33, 229, 50, 308, 16, 515, 33, 589, 50, ...\n",
            "8   10951  [1, 67, 172, 73, 349, 73, 526, 69, 708, 61, 88...\n",
            "9   10980  [179, 63, 691, 63, 1203, 63, 1706, 94, 2214, 9...\n"
          ]
        }
      ],
      "source": [
        "# ==================== GENERATE SUBMISSION ====================\n",
        "\n",
        "rows = []\n",
        "test_files = sorted(os.listdir(TEST_DIR))\n",
        "print(f\"Processing {len(test_files)} test images...\")\n",
        "\n",
        "for f in tqdm(test_files, desc=\"Inference\"):\n",
        "    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n",
        "    label, masks, dbg = pipeline_final(pil)\n",
        "\n",
        "    if label == \"authentic\" or len(masks) == 0:\n",
        "        annot = \"authentic\"\n",
        "    else:\n",
        "        annot = rle_encode_multi(masks)\n",
        "\n",
        "    rows.append({\n",
        "        \"case_id\": Path(f).stem,\n",
        "        \"annotation\": annot,\n",
        "    })\n",
        "\n",
        "# Create submission DataFrame\n",
        "sub = pd.DataFrame(rows)\n",
        "\n",
        "if LOCAL_MODE or SAMPLE_SUB is None:\n",
        "    # Local mode: just use our predictions directly\n",
        "    final = sub\n",
        "else:\n",
        "    # Kaggle mode: merge with sample submission to ensure correct order\n",
        "    ss = pd.read_csv(SAMPLE_SUB)\n",
        "    ss[\"case_id\"] = ss[\"case_id\"].astype(str)\n",
        "    sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n",
        "    final = ss[[\"case_id\"]].merge(sub, on=\"case_id\", how=\"left\")\n",
        "    final[\"annotation\"] = final[\"annotation\"].fillna(\"authentic\")\n",
        "\n",
        "# Save\n",
        "final[[\"case_id\", \"annotation\"]].to_csv(OUT_PATH, index=False)\n",
        "\n",
        "print(f\"\\n✅ Saved submission to: {OUT_PATH}\")\n",
        "print(f\"Total rows: {len(final)}\")\n",
        "print(f\"Forged: {(final['annotation'] != 'authentic').sum()}\")\n",
        "print(f\"Authentic: {(final['annotation'] == 'authentic').sum()}\")\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "print(final.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "LOCAL SCORING - Comparing with ground truth\n",
            "============================================================\n",
            "\n",
            "Prediction stats:\n",
            "  Total images: 929\n",
            "  Predicted forged: 706\n",
            "  Predicted authentic: 223\n",
            "\n",
            "Mask count stats (forged images only):\n",
            "  Min masks: 1\n",
            "  Max masks: 2\n",
            "  Mean masks: 1.36\n"
          ]
        }
      ],
      "source": [
        "# ==================== LOCAL SCORING (only in LOCAL_MODE) ====================\n",
        "\n",
        "if LOCAL_MODE:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"LOCAL SCORING - Comparing with ground truth\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # You can add scoring code here if you want to evaluate locally\n",
        "    # For now, just show some stats\n",
        "    \n",
        "    print(f\"\\nPrediction stats:\")\n",
        "    print(f\"  Total images: {len(final)}\")\n",
        "    print(f\"  Predicted forged: {(final['annotation'] != 'authentic').sum()}\")\n",
        "    print(f\"  Predicted authentic: {(final['annotation'] == 'authentic').sum()}\")\n",
        "    \n",
        "    # Count masks per forged image\n",
        "    forged_rows = final[final['annotation'] != 'authentic']\n",
        "    if len(forged_rows) > 0:\n",
        "        mask_counts = forged_rows['annotation'].apply(lambda x: len(x.split(';')))\n",
        "        print(f\"\\nMask count stats (forged images only):\")\n",
        "        print(f\"  Min masks: {mask_counts.min()}\")\n",
        "        print(f\"  Max masks: {mask_counts.max()}\")\n",
        "        print(f\"  Mean masks: {mask_counts.mean():.2f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
