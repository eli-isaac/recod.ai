{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Score Submission\n",
        "\n",
        "This notebook scores the submission.csv generated by submission.ipynb against the validation ground truth.\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Run the \"Prepare Fake Kaggle Environment\" cell in cnn-dinov2-hybrid.ipynb\n",
        "2. Run submission.ipynb to generate submission.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numba\n",
        "import scipy.optimize\n",
        "import numpy.typing as npt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== PATHS ====================\n",
        "SUBMISSION_PATH = \"submission.csv\"\n",
        "GT_DIR = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/ground_truth\"\n",
        "GT_CSV = os.path.join(GT_DIR, \"ground_truth.csv\")\n",
        "\n",
        "# Check files exist\n",
        "assert os.path.exists(SUBMISSION_PATH), f\"submission.csv not found. Run submission.ipynb first!\"\n",
        "assert os.path.exists(GT_CSV), f\"Ground truth not found. Run the setup cell in cnn-dinov2-hybrid.ipynb first!\"\n",
        "\n",
        "print(f\"‚úÖ Found submission.csv\")\n",
        "print(f\"‚úÖ Found ground truth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== RLE DECODE ====================\n",
        "\n",
        "@numba.njit\n",
        "def _rle_decode_jit(mask_rle: npt.NDArray, height: int, width: int) -> npt.NDArray:\n",
        "    if len(mask_rle) % 2 != 0:\n",
        "        raise ValueError('Odd number of values')\n",
        "    starts, lengths = mask_rle[0::2], mask_rle[1::2]\n",
        "    starts = starts - 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(height * width, dtype=np.bool_)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img\n",
        "\n",
        "\n",
        "def rle_decode(mask_rle: str, shape: tuple) -> npt.NDArray:\n",
        "    mask_rle = json.loads(mask_rle)\n",
        "    mask_rle = np.asarray(mask_rle, dtype=np.int32)\n",
        "    return _rle_decode_jit(mask_rle, shape[0], shape[1]).reshape(shape, order='F')\n",
        "\n",
        "\n",
        "@numba.jit(nopython=True)\n",
        "def _rle_encode_jit(x: npt.NDArray, fg_val: int = 1):\n",
        "    dots = np.where(x.T.flatten() == fg_val)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "\n",
        "def rle_encode(masks: list, fg_val: int = 1) -> str:\n",
        "    return ';'.join([json.dumps(_rle_encode_jit(x, fg_val)) for x in masks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== SCORING FUNCTIONS ====================\n",
        "\n",
        "def calculate_f1_score(pred_mask: npt.NDArray, gt_mask: npt.NDArray):\n",
        "    pred_flat = pred_mask.flatten()\n",
        "    gt_flat = gt_mask.flatten()\n",
        "    tp = np.sum((pred_flat == 1) & (gt_flat == 1))\n",
        "    fp = np.sum((pred_flat == 1) & (gt_flat == 0))\n",
        "    fn = np.sum((pred_flat == 0) & (gt_flat == 1))\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    if (precision + recall) > 0:\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def calculate_f1_matrix(pred_masks: list, gt_masks: list):\n",
        "    num_pred = len(pred_masks)\n",
        "    num_gt = len(gt_masks)\n",
        "    f1_matrix = np.zeros((max(num_pred, num_gt), num_gt))\n",
        "    for i in range(num_pred):\n",
        "        for j in range(num_gt):\n",
        "            f1_matrix[i, j] = calculate_f1_score(pred_masks[i], gt_masks[j])\n",
        "    return f1_matrix\n",
        "\n",
        "\n",
        "def oF1_score(pred_masks: list, gt_masks: list):\n",
        "    if len(pred_masks) == 0 or len(gt_masks) == 0:\n",
        "        return 0.0\n",
        "    f1_matrix = calculate_f1_matrix(pred_masks, gt_masks)\n",
        "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-f1_matrix)\n",
        "    excess_penalty = len(gt_masks) / max(len(pred_masks), len(gt_masks))\n",
        "    return np.mean(f1_matrix[row_ind, col_ind]) * excess_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== LOAD DATA ====================\n",
        "\n",
        "# Load submission\n",
        "submission_df = pd.read_csv(SUBMISSION_PATH)\n",
        "submission_df['case_id'] = submission_df['case_id'].astype(str)\n",
        "print(f\"Loaded submission: {len(submission_df)} rows\")\n",
        "\n",
        "# Load ground truth\n",
        "gt_df = pd.read_csv(GT_CSV)\n",
        "gt_df['case_id'] = gt_df['case_id'].astype(str)\n",
        "print(f\"Loaded ground truth: {len(gt_df)} rows\")\n",
        "\n",
        "# Merge\n",
        "merged = gt_df.merge(submission_df, on='case_id', how='left')\n",
        "merged['annotation'] = merged['annotation'].fillna('authentic')\n",
        "print(f\"Merged: {len(merged)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== SCORE ====================\n",
        "\n",
        "scores = []\n",
        "details = []\n",
        "\n",
        "for idx, row in tqdm(merged.iterrows(), total=len(merged), desc=\"Scoring\"):\n",
        "    case_id = row['case_id']\n",
        "    gt_label = row['label']\n",
        "    pred_annotation = row['annotation']\n",
        "    shape = json.loads(row['shape']) if isinstance(row['shape'], str) else row['shape']\n",
        "    \n",
        "    # Case 1: Both authentic\n",
        "    if gt_label == 'authentic' and pred_annotation == 'authentic':\n",
        "        score = 1.0\n",
        "        detail = \"TN (both authentic)\"\n",
        "    \n",
        "    # Case 2: GT authentic, Pred forged (false positive)\n",
        "    elif gt_label == 'authentic' and pred_annotation != 'authentic':\n",
        "        score = 0.0\n",
        "        detail = \"FP (predicted forged, actually authentic)\"\n",
        "    \n",
        "    # Case 3: GT forged, Pred authentic (false negative)\n",
        "    elif gt_label == 'forged' and pred_annotation == 'authentic':\n",
        "        score = 0.0\n",
        "        detail = \"FN (predicted authentic, actually forged)\"\n",
        "    \n",
        "    # Case 4: Both forged - compute oF1\n",
        "    else:\n",
        "        # Load ground truth masks\n",
        "        mask_file = row['mask_file']\n",
        "        gt_masks_arr = np.load(os.path.join(GT_DIR, mask_file))\n",
        "        if gt_masks_arr.ndim == 2:\n",
        "            gt_masks = [gt_masks_arr]\n",
        "        else:\n",
        "            gt_masks = [gt_masks_arr[i] for i in range(gt_masks_arr.shape[0])]\n",
        "        \n",
        "        # Decode predicted masks\n",
        "        pred_rles = pred_annotation.split(';')\n",
        "        pred_masks = [rle_decode(rle, tuple(shape)) for rle in pred_rles]\n",
        "        \n",
        "        score = oF1_score(pred_masks, gt_masks)\n",
        "        detail = f\"oF1 (pred={len(pred_masks)} masks, gt={len(gt_masks)} masks)\"\n",
        "    \n",
        "    scores.append(score)\n",
        "    details.append(detail)\n",
        "\n",
        "merged['score'] = scores\n",
        "merged['detail'] = details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== RESULTS ====================\n",
        "\n",
        "final_score = np.mean(scores)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìä FINAL SCORE: {final_score:.4f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Breakdown\n",
        "print(f\"\\nüìà Score Breakdown:\")\n",
        "print(f\"   Total images: {len(merged)}\")\n",
        "print(f\"   Mean score: {final_score:.4f}\")\n",
        "print(f\"   Std score: {np.std(scores):.4f}\")\n",
        "print(f\"   Min score: {np.min(scores):.4f}\")\n",
        "print(f\"   Max score: {np.max(scores):.4f}\")\n",
        "\n",
        "# By category\n",
        "print(f\"\\nüìã By Category:\")\n",
        "for detail_type in merged['detail'].unique():\n",
        "    subset = merged[merged['detail'] == detail_type]\n",
        "    print(f\"   {detail_type}: {len(subset)} images, avg score = {subset['score'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== WORST PREDICTIONS ====================\n",
        "\n",
        "print(\"\\n‚ùå Worst 10 Predictions:\")\n",
        "worst = merged.nsmallest(10, 'score')[['case_id', 'label', 'score', 'detail']]\n",
        "print(worst.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CONFUSION MATRIX ====================\n",
        "\n",
        "# Simplified: authentic vs forged prediction\n",
        "merged['pred_label'] = merged['annotation'].apply(lambda x: 'authentic' if x == 'authentic' else 'forged')\n",
        "\n",
        "print(\"\\nüî¢ Confusion Matrix (Classification):\")\n",
        "print(pd.crosstab(merged['label'], merged['pred_label'], margins=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
