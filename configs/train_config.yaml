# Training Configuration
#
# Usage:
#   python scripts/train.py --config configs/train_config.yaml

model:
  backbone: "facebook/dinov2-base"
  img_size: 512
  channels: 4  # Number of mask channels
  unfreeze_blocks: 4  # Number of transformer blocks to fine-tune
  decoder_dropout: 0.15

data:
  # HuggingFace dataset IDs
  datasets:
    - "eliplutchok/recod-pretrain-1"
    - "eliplutchok/recod-pretrain-2"
    - "eliplutchok/recod-pretrain-3"
    # - "eliplutchok/recod-finetune"
  
  num_workers: 12
  val_split: 0.10

training:
  batch_size: 64
  learning_rate: 3.0e-4
  backbone_lr_scale: 0.033  # Backbone gets ~30x smaller LR
  weight_decay: 0.01
  epochs: 40
  early_stopping_patience: 8
  # pos_weight: 86.0  # Computed from dataset
  
  best_model_metric: "f1"
  
  scheduler:
    type: "cosine"
    warmup_epochs: 3
    min_lr: 1.0e-6

  save_every: 5
  sample_every: 5  # Generate sample predictions every N epochs
  checkpoint_dir: "checkpoints"
  
  # Resume/finetune (optional)
  # resume_from: "checkpoints/checkpoint_epoch_10.pt"  # Continue interrupted training
  # weights_from: "checkpoints/best_model.pt"          # Finetune with new config

logging:
  log_dir: "outputs/logs"
  wandb_project: null
  log_every_n_steps: 10

seed: 42
